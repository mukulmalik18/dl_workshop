{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bf/anaconda3/envs/factnlp/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are keeping 20% of data samples as test set\n",
    "train_X, test_X, train_y, test_y = train_test_split(data, target, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 30, 120, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X), len(test_X), len(train_y), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idim = train_X[0].shape[0] # size of input layer - \"4\"\n",
    "hdim = 100 # size of hidden layers(100 nodes)\n",
    "odim = len(np.unique(train_y)) # size of output layer - \"3\"\n",
    "\n",
    "alpha = 0.001 # Learning rate\n",
    "reg_lambda = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = {'W1': None, 'b1': None, 'W2': None, 'b2': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_prop(model, x):\n",
    "    \n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores/np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(model, x):\n",
    "    \n",
    "    probs = forward_prop(model, x)\n",
    "    \n",
    "    targets = -np.log(probs[range(len(train_X)), train_y])\n",
    "    loss = np.sum(targets)\n",
    "    \n",
    "    loss += reg_lambda/2 * (np.sum(np.square(model['W1'])) + np.sum(np.square(model['W2'])))\n",
    "    return 1./len(train_X) * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    probs = forward_prop(model, x)\n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "def get_accuracy(model, x, y):    \n",
    "    predictions = predict(model, x)\n",
    "    accuracy = np.sum(y == predictions)/len(x)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainer(hdim, epochs):\n",
    "    \n",
    "    W1 = np.random.rand(idim, hdim)/np.sqrt(idim)\n",
    "    b1 = np.zeros((1, hdim))\n",
    "    W2 = np.random.randn(hdim, odim)/np.sqrt(hdim)\n",
    "    b2 = np.zeros((1, odim))\n",
    "    \n",
    "    model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    \n",
    "    # For whole batch\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        z1 = train_X.dot(W1) + b1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        exp_scores = np.exp(z2)\n",
    "        probs = exp_scores/np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        # Backpropagation\n",
    "        delta3 = probs\n",
    "        delta3[range(len(train_X)), train_y] -= 1\n",
    "        dW2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "        dW1 = np.dot(train_X.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "        \n",
    "        # Add regularization terms\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "        \n",
    "        # Gradient descent\n",
    "        W1 += -alpha * dW1\n",
    "        b1 += -alpha * db1\n",
    "        W2 += -alpha * dW2\n",
    "        b2 += -alpha * db2\n",
    "        \n",
    "        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        print(\"Loss after iteration %d: %f\"%(epoch, get_loss(model, train_X)))\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Accuracy after iteration %d: %f\"%(epoch, get_accuracy(model, test_X, test_y)))\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.730792\n",
      "Accuracy after iteration 0: 0.500000\n",
      "Loss after iteration 1: 1.439260\n",
      "Accuracy after iteration 1: 0.500000\n",
      "Loss after iteration 2: 1.279237\n",
      "Accuracy after iteration 2: 0.500000\n",
      "Loss after iteration 3: 1.185159\n",
      "Accuracy after iteration 3: 0.566667\n",
      "Loss after iteration 4: 1.128534\n",
      "Accuracy after iteration 4: 0.700000\n",
      "Loss after iteration 5: 1.094806\n",
      "Accuracy after iteration 5: 0.266667\n",
      "Loss after iteration 6: 1.073356\n",
      "Accuracy after iteration 6: 0.266667\n",
      "Loss after iteration 7: 1.056066\n",
      "Accuracy after iteration 7: 0.266667\n",
      "Loss after iteration 8: 1.036318\n",
      "Accuracy after iteration 8: 0.333333\n",
      "Loss after iteration 9: 1.007945\n",
      "Accuracy after iteration 9: 0.500000\n",
      "Loss after iteration 10: 0.969829\n",
      "Accuracy after iteration 10: 0.500000\n",
      "Loss after iteration 11: 0.936348\n",
      "Accuracy after iteration 11: 0.500000\n",
      "Loss after iteration 12: 0.912907\n",
      "Accuracy after iteration 12: 0.500000\n",
      "Loss after iteration 13: 0.891245\n",
      "Accuracy after iteration 13: 0.500000\n",
      "Loss after iteration 14: 0.869881\n",
      "Accuracy after iteration 14: 0.500000\n",
      "Loss after iteration 15: 0.848784\n",
      "Accuracy after iteration 15: 0.500000\n",
      "Loss after iteration 16: 0.828136\n",
      "Accuracy after iteration 16: 0.500000\n",
      "Loss after iteration 17: 0.808122\n",
      "Accuracy after iteration 17: 0.500000\n",
      "Loss after iteration 18: 0.788880\n",
      "Accuracy after iteration 18: 0.500000\n",
      "Loss after iteration 19: 0.770502\n",
      "Accuracy after iteration 19: 0.500000\n",
      "Loss after iteration 20: 0.753050\n",
      "Accuracy after iteration 20: 0.500000\n",
      "Loss after iteration 21: 0.736553\n",
      "Accuracy after iteration 21: 0.500000\n",
      "Loss after iteration 22: 0.721018\n",
      "Accuracy after iteration 22: 0.500000\n",
      "Loss after iteration 23: 0.706429\n",
      "Accuracy after iteration 23: 0.500000\n",
      "Loss after iteration 24: 0.692759\n",
      "Accuracy after iteration 24: 0.500000\n",
      "Loss after iteration 25: 0.679969\n",
      "Accuracy after iteration 25: 0.500000\n",
      "Loss after iteration 26: 0.668014\n",
      "Accuracy after iteration 26: 0.500000\n",
      "Loss after iteration 27: 0.656845\n",
      "Accuracy after iteration 27: 0.500000\n",
      "Loss after iteration 28: 0.646409\n",
      "Accuracy after iteration 28: 0.500000\n",
      "Loss after iteration 29: 0.636655\n",
      "Accuracy after iteration 29: 0.500000\n",
      "Loss after iteration 30: 0.627533\n",
      "Accuracy after iteration 30: 0.500000\n",
      "Loss after iteration 31: 0.618994\n",
      "Accuracy after iteration 31: 0.500000\n",
      "Loss after iteration 32: 0.610991\n",
      "Accuracy after iteration 32: 0.500000\n",
      "Loss after iteration 33: 0.603481\n",
      "Accuracy after iteration 33: 0.500000\n",
      "Loss after iteration 34: 0.596422\n",
      "Accuracy after iteration 34: 0.500000\n",
      "Loss after iteration 35: 0.589776\n",
      "Accuracy after iteration 35: 0.500000\n",
      "Loss after iteration 36: 0.583509\n",
      "Accuracy after iteration 36: 0.500000\n",
      "Loss after iteration 37: 0.577586\n",
      "Accuracy after iteration 37: 0.500000\n",
      "Loss after iteration 38: 0.571978\n",
      "Accuracy after iteration 38: 0.500000\n",
      "Loss after iteration 39: 0.566657\n",
      "Accuracy after iteration 39: 0.500000\n",
      "Loss after iteration 40: 0.561597\n",
      "Accuracy after iteration 40: 0.500000\n",
      "Loss after iteration 41: 0.556775\n",
      "Accuracy after iteration 41: 0.500000\n",
      "Loss after iteration 42: 0.552167\n",
      "Accuracy after iteration 42: 0.500000\n",
      "Loss after iteration 43: 0.547755\n",
      "Accuracy after iteration 43: 0.500000\n",
      "Loss after iteration 44: 0.543520\n",
      "Accuracy after iteration 44: 0.500000\n",
      "Loss after iteration 45: 0.539444\n",
      "Accuracy after iteration 45: 0.500000\n",
      "Loss after iteration 46: 0.535510\n",
      "Accuracy after iteration 46: 0.500000\n",
      "Loss after iteration 47: 0.531706\n",
      "Accuracy after iteration 47: 0.500000\n",
      "Loss after iteration 48: 0.528015\n",
      "Accuracy after iteration 48: 0.500000\n",
      "Loss after iteration 49: 0.524428\n",
      "Accuracy after iteration 49: 0.500000\n",
      "Loss after iteration 50: 0.520930\n",
      "Accuracy after iteration 50: 0.500000\n",
      "Loss after iteration 51: 0.517513\n",
      "Accuracy after iteration 51: 0.500000\n",
      "Loss after iteration 52: 0.514166\n",
      "Accuracy after iteration 52: 0.500000\n",
      "Loss after iteration 53: 0.510880\n",
      "Accuracy after iteration 53: 0.500000\n",
      "Loss after iteration 54: 0.507648\n",
      "Accuracy after iteration 54: 0.500000\n",
      "Loss after iteration 55: 0.504461\n",
      "Accuracy after iteration 55: 0.500000\n",
      "Loss after iteration 56: 0.501313\n",
      "Accuracy after iteration 56: 0.500000\n",
      "Loss after iteration 57: 0.498199\n",
      "Accuracy after iteration 57: 0.500000\n",
      "Loss after iteration 58: 0.495112\n",
      "Accuracy after iteration 58: 0.533333\n",
      "Loss after iteration 59: 0.492049\n",
      "Accuracy after iteration 59: 0.533333\n",
      "Loss after iteration 60: 0.489004\n",
      "Accuracy after iteration 60: 0.533333\n",
      "Loss after iteration 61: 0.485974\n",
      "Accuracy after iteration 61: 0.566667\n",
      "Loss after iteration 62: 0.482956\n",
      "Accuracy after iteration 62: 0.566667\n",
      "Loss after iteration 63: 0.479947\n",
      "Accuracy after iteration 63: 0.600000\n",
      "Loss after iteration 64: 0.476944\n",
      "Accuracy after iteration 64: 0.633333\n",
      "Loss after iteration 65: 0.473944\n",
      "Accuracy after iteration 65: 0.666667\n",
      "Loss after iteration 66: 0.470947\n",
      "Accuracy after iteration 66: 0.700000\n",
      "Loss after iteration 67: 0.467950\n",
      "Accuracy after iteration 67: 0.733333\n",
      "Loss after iteration 68: 0.464951\n",
      "Accuracy after iteration 68: 0.733333\n",
      "Loss after iteration 69: 0.461950\n",
      "Accuracy after iteration 69: 0.733333\n",
      "Loss after iteration 70: 0.458945\n",
      "Accuracy after iteration 70: 0.733333\n",
      "Loss after iteration 71: 0.455936\n",
      "Accuracy after iteration 71: 0.766667\n",
      "Loss after iteration 72: 0.452922\n",
      "Accuracy after iteration 72: 0.766667\n",
      "Loss after iteration 73: 0.449902\n",
      "Accuracy after iteration 73: 0.766667\n",
      "Loss after iteration 74: 0.446876\n",
      "Accuracy after iteration 74: 0.766667\n",
      "Loss after iteration 75: 0.443844\n",
      "Accuracy after iteration 75: 0.766667\n",
      "Loss after iteration 76: 0.440805\n",
      "Accuracy after iteration 76: 0.766667\n",
      "Loss after iteration 77: 0.437759\n",
      "Accuracy after iteration 77: 0.766667\n",
      "Loss after iteration 78: 0.434706\n",
      "Accuracy after iteration 78: 0.766667\n",
      "Loss after iteration 79: 0.431647\n",
      "Accuracy after iteration 79: 0.766667\n",
      "Loss after iteration 80: 0.428581\n",
      "Accuracy after iteration 80: 0.766667\n",
      "Loss after iteration 81: 0.425509\n",
      "Accuracy after iteration 81: 0.766667\n",
      "Loss after iteration 82: 0.422431\n",
      "Accuracy after iteration 82: 0.800000\n",
      "Loss after iteration 83: 0.419347\n",
      "Accuracy after iteration 83: 0.833333\n",
      "Loss after iteration 84: 0.416259\n",
      "Accuracy after iteration 84: 0.833333\n",
      "Loss after iteration 85: 0.413165\n",
      "Accuracy after iteration 85: 0.833333\n",
      "Loss after iteration 86: 0.410068\n",
      "Accuracy after iteration 86: 0.833333\n",
      "Loss after iteration 87: 0.406967\n",
      "Accuracy after iteration 87: 0.866667\n",
      "Loss after iteration 88: 0.403863\n",
      "Accuracy after iteration 88: 0.866667\n",
      "Loss after iteration 89: 0.400757\n",
      "Accuracy after iteration 89: 0.866667\n",
      "Loss after iteration 90: 0.397650\n",
      "Accuracy after iteration 90: 0.866667\n",
      "Loss after iteration 91: 0.394542\n",
      "Accuracy after iteration 91: 0.900000\n",
      "Loss after iteration 92: 0.391434\n",
      "Accuracy after iteration 92: 0.900000\n",
      "Loss after iteration 93: 0.388327\n",
      "Accuracy after iteration 93: 0.900000\n",
      "Loss after iteration 94: 0.385222\n",
      "Accuracy after iteration 94: 0.900000\n",
      "Loss after iteration 95: 0.382120\n",
      "Accuracy after iteration 95: 0.900000\n",
      "Loss after iteration 96: 0.379022\n",
      "Accuracy after iteration 96: 0.900000\n",
      "Loss after iteration 97: 0.375928\n",
      "Accuracy after iteration 97: 0.933333\n",
      "Loss after iteration 98: 0.372839\n",
      "Accuracy after iteration 98: 0.933333\n",
      "Loss after iteration 99: 0.369757\n",
      "Accuracy after iteration 99: 0.933333\n"
     ]
    }
   ],
   "source": [
    "model = trainer(8, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:factnlp]",
   "language": "python",
   "name": "conda-env-factnlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
