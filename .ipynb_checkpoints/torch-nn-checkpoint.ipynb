{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f00b8141df8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.2000\n",
       " 0.5000\n",
       " 0.7000\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a torch.Tensor object with the given data.\n",
    "T_data = [0.2, 0.5, 0.7]\n",
    "T = torch.Tensor(T_data)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.2000  0.5000  0.7000\n",
       " 1.0000  2.0000  3.0000\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a torch.Tensor object with the given data.\n",
    "T_data = [[0.2, 0.5, 0.7], [1., 2., 3.]]\n",
    "T = torch.Tensor(T_data)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.2000\n",
       " 0.5000\n",
       " 0.7000\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       " -2.9718  1.7070 -0.4305 -2.2820  0.5237\n",
       "  0.0004 -1.2039  3.5283  0.4434  0.5848\n",
       "  0.8407  0.5510  0.3863  0.9124 -0.8410\n",
       "  1.2282 -1.8661  1.4146 -1.8781 -0.4674\n",
       "\n",
       "(1 ,.,.) = \n",
       " -0.7576  0.4215 -0.4827 -1.1198  0.3056\n",
       "  1.0386  0.5206 -0.5006  1.2182  0.2117\n",
       " -1.0613 -1.9441 -0.9596  0.5489 -0.9901\n",
       " -0.3826  1.5037  1.8267  0.5561  1.6445\n",
       "\n",
       "(2 ,.,.) = \n",
       "  0.4973 -1.5067  1.7661 -0.3569 -0.1713\n",
       "  0.4068 -0.4284 -1.1299  1.4274 -1.4027\n",
       "  1.4825 -1.1559  1.6190  0.9581  0.7747\n",
       "  0.1940  0.1687  0.3061  1.0743 -1.0327\n",
       "[torch.FloatTensor of size 3x4x5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensor with random data\n",
    "x = torch.randn((3, 4, 5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([1., 2., 3.])\n",
    "y = torch.Tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "  1.0930  0.7769 -1.3128  0.7099\n",
      "  0.9944 -0.2694 -0.6491 -0.1373\n",
      " -0.2954 -0.7725 -0.2215  0.5074\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.6794 -1.6115  0.5230 -0.8890\n",
      "  0.2620  0.0302  0.0013 -1.3987\n",
      "  1.4666 -0.1028 -0.0097 -0.8420\n",
      "[torch.FloatTensor of size 2x3x4]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      " 1.0930  0.7769 -1.3128  0.7099  0.9944 -0.2694 -0.6491 -0.1373 -0.2954 -0.7725\n",
      "-0.6794 -1.6115  0.5230 -0.8890  0.2620  0.0302  0.0013 -1.3987  1.4666 -0.1028\n",
      "\n",
      "Columns 10 to 11 \n",
      "-0.2215  0.5074\n",
      "-0.0097 -0.8420\n",
      "[torch.FloatTensor of size 2x12]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      " 1.0930  0.7769 -1.3128  0.7099  0.9944 -0.2694 -0.6491 -0.1373 -0.2954 -0.7725\n",
      "-0.6794 -1.6115  0.5230 -0.8890  0.2620  0.0302  0.0013 -1.3987  1.4666 -0.1028\n",
      "\n",
      "Columns 10 to 11 \n",
      "-0.2215  0.5074\n",
      "-0.0097 -0.8420\n",
      "[torch.FloatTensor of size 2x12]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reshaping\n",
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "<torch.autograd.function.AddBackward object at 0x7f006c977318>\n"
     ]
    }
   ],
   "source": [
    "# The Variable class keeps track of how it was created. \n",
    "x = autograd.Variable(torch.Tensor([1., 2., 3]), requires_grad=True)\n",
    "# You can access the data with the .data attribute\n",
    "print(x.data)\n",
    "\n",
    "# You can also do all the same operations you did with tensors with Variables.\n",
    "y = autograd.Variable(torch.Tensor([4., 5., 6]), requires_grad=True)\n",
    "z = x + y\n",
    "print(z.data)\n",
    "\n",
    "# BUT z knows something extra.\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 21\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "<torch.autograd.function.SumBackward object at 0x7f006c977228>\n"
     ]
    }
   ],
   "source": [
    "# Lets sum up all the entries in z\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calling .backward() on any variable will run backprop, starting from it.\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable as Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "X_WORDS = {'unknown': \"<UNK>\"}\n",
    "\n",
    "def data_word_mapping(documents):\n",
    "    \"\"\"\n",
    "    Returns unique words in a list of strings\n",
    "    \n",
    "    :param documents: list: a list of strings    \n",
    "    :returns (None, index_to_word: list: Index to word mapping,\n",
    "              word_to_index: dict: Word to Index mapping)\n",
    "    \"\"\"\n",
    "    \n",
    "    # If type of documents is a list of words then join them together\n",
    "    if type(documents[0]) == list:\n",
    "        documents = [\" \".join(doc) for doc in documents]\n",
    "        \n",
    "    vocab = (\" \".join(documents).split()) + [X_WORDS[\"unknown\"]] # End tags will already be there\n",
    "    index_to_word = np.unique(vocab)\n",
    "    word_to_index = dict((word, idx) for idx, word in enumerate(index_to_word))\n",
    "    \n",
    "    return index_to_word, word_to_index\n",
    "\n",
    "index_to_word, word_to_index = data_word_mapping([sample[0] for sample in train_data])\n",
    "index_to_tag, tag_to_index = ['SPANISH', 'ENGLISH'], {'SPANISH': 0, 'ENGLISH': 1}\n",
    "\n",
    "VOCAB_SIZE = len(word_to_index)\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, hidden_dim, num_labels, vocab_size, embedding_dim = 4):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.i2h = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.h2o = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def avg_vectors(self, vectors):\n",
    "        \"\"\"\n",
    "        Get the average vector for a document.\n",
    "        \n",
    "        :param: vectors: torch.FloatTensor: A tensor with word vector for each word\n",
    "        :returns torch.FloatTensor\n",
    "        \"\"\"\n",
    "        return torch.mean(vectors, dim=0)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"\n",
    "        Forward-Pass for LSTM, which returns the probability scores of classes. \n",
    "        \n",
    "        :param tokens: autograd.Variable: a list of indices as torch tensors\n",
    "        \n",
    "        :returns: label_scores: autograd.Variable: probability scores for classes\n",
    "        \"\"\"\n",
    "        embeds = self.word_embeddings(tokens)\n",
    "        output = self.i2h(self.avg_vectors(embeds)).view((1, -1))\n",
    "        final_output = self.h2o(F.relu(output))\n",
    "        label_probs = F.log_softmax(final_output)\n",
    "        return label_probs\n",
    "\n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_indices(document, to_ix):\n",
    "    \"\"\"\n",
    "    Converts documents into a list of indices.\n",
    "    \n",
    "    :param documents: list: of list: of str: a list of lists of words\n",
    "    :param to_ix: dict: a word to index mapping\n",
    "    :returns indices: list: of list: of int: a list of lists of word indices\n",
    "    \"\"\"    \n",
    "    indices = list()\n",
    "        \n",
    "    for word in document:\n",
    "        try:\n",
    "            # Look for the word in dict\n",
    "            indices.append(to_ix[word])\n",
    "        except:\n",
    "            # If not found then add a special word for unknown\n",
    "            indices.append(to_ix[X_WORDS[\"unknown\"]])\n",
    "        \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(to_indices(x, word_to_index),\n",
    "               to_indices([y], tag_to_index)) for x, y in train_data]\n",
    "test_data = [(to_indices(x, word_to_index),\n",
    "               to_indices([y], tag_to_index)) for x, y in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([18, 12, 7, 9, 16, 6], [0]),\n",
       " ([1, 15, 22, 18], [1]),\n",
       " ([2, 8, 20, 21, 23, 5, 13], [0]),\n",
       " ([2, 15, 14, 19, 3, 11, 13, 22, 10, 17, 4, 21], [1])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(hidden_dim=8, num_labels=len(index_to_tag),\n",
    "           vocab_size=VOCAB_SIZE, embedding_dim=16)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01,\n",
    "                                          weight_decay=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    \"\"\"\n",
    "    Get the prediction as the class name from trained model.\n",
    "    \n",
    "    :param model: pytorch model\n",
    "    :param x: str: a test document\n",
    "    \n",
    "    :returns tag: int: class id for the input\n",
    "    \"\"\"\n",
    "    x = Variable(torch.LongTensor(x))\n",
    "    yhat = model(x)\n",
    "    _, tag = yhat.max(1)\n",
    "    \n",
    "    return tag.data.numpy()\n",
    "\n",
    "def get_accuracy(eval_data):\n",
    "    inputs = [sample[0] for sample in eval_data]\n",
    "    results = [predict(x) for x in inputs]\n",
    "    x = \n",
    "    accuracy = np.sum(x == y)/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1]), array([0])]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-83d27bc0c84d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch %d - Test Accuracy: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AVERAGE EPOCH LOSS AFTER EPOCH %d: %f\"\u001b[0m\u001b[0;34m%\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    \n",
    "    epoch_loss = 0 \n",
    "    \n",
    "    for tokens, labels in train_data:\n",
    "        \n",
    "        # Initialize hidden states to zero\n",
    "        model.zero_grad()\n",
    "                \n",
    "        x, y = Variable(torch.LongTensor(tokens)), Variable(torch.LongTensor(labels))     \n",
    "        \n",
    "        y_ = model(x)        \n",
    "        loss = loss_fn(y_, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value = loss.data.numpy()\n",
    "        epoch_loss += loss_value\n",
    "    \n",
    "    print(\"Epoch %d - Test Accuracy: %f\" % (epoch, get_accuracy(test_data)))\n",
    "    print(\"AVERAGE EPOCH LOSS AFTER EPOCH %d: %f\"%  (epoch, epoch_loss/len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_accuracy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:factnlp]",
   "language": "python",
   "name": "conda-env-factnlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
