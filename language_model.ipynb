{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en_core_web_md'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable as Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from scripts.preprocess import *\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.models import Word2Vec as word2vec\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dict of some additional special words\n",
    "X_WORDS = {\"unknown\": \"<unk>\", \"start\": \"<start>\", \"end\": \"<end>\", \"digit\": \"<digit>\"}\n",
    "\n",
    "\n",
    "def add_boundary_tags(tokens):\n",
    "    \"\"\"\n",
    "    Adds start and end tags to list of tokens\n",
    "    \n",
    "    :param tokens: list: list of tokenized words\n",
    "    :returns str: [<start>, w1, w2...., wn, <end>]\n",
    "    \"\"\"\n",
    "    return [X_WORDS[\"start\"]] + tokens + [X_WORDS[\"end\"]]\n",
    "\n",
    "\n",
    "def preprocess(documents, to_lower=True, boundary_tags=False):\n",
    "    \"\"\"\n",
    "    Preprocesses raw text - convert into lowercase add boundary tags\n",
    "    \n",
    "    :param documents: list: of str\n",
    "    :param to_lower: bool: whether to convert text into lowercase(default=True)\n",
    "    :param boundary_tags: bool: whether to keep boundary tags or not(start, end)\n",
    "    :returns processed: list: of list: of str: a list of lists of words\n",
    "    \"\"\"\n",
    "    processed = list() \n",
    "    \n",
    "    for doc in documents:\n",
    "        \n",
    "        # Convert into lowercase if flag is set\n",
    "        if to_lower:\n",
    "            doc = doc.lower()\n",
    "        tokens = [c for c in doc]\n",
    "        if boundary_tags:\n",
    "            tokens = add_boundary_tags(tokens)\n",
    "        processed.append(tokens)\n",
    "        \n",
    "    return processed\n",
    "\n",
    "\n",
    "def to_indices(document, to_ix):\n",
    "    \"\"\"\n",
    "    Converts documents into a list of indices.\n",
    "    \n",
    "    :param documents: list: of list: of str: a list of lists of words\n",
    "    :param to_ix: dict: a word to index mapping\n",
    "    :returns indices: list: of list: of int: a list of lists of word indices\n",
    "    \"\"\"    \n",
    "    indices = list()\n",
    "        \n",
    "    for word in document:\n",
    "        try:\n",
    "            # Look for the word in dict\n",
    "            indices.append(to_ix[word])\n",
    "        except:\n",
    "            # If not found then add a special word for unknown\n",
    "            indices.append(to_ix[X_WORDS[\"unknown\"]])\n",
    "        \n",
    "    return indices\n",
    "\n",
    "\n",
    "def w2v_word_mapping(model_path):\n",
    "    \"\"\"\n",
    "    Returns mapping of words to indices and vice-versa.\n",
    "    In addition to a numpy matrix representation of\n",
    "    pre-trained word vectors with gensim.\n",
    "    \n",
    "    :param model_path: str: Relative path to the pre-trained gensim model    \n",
    "    :returns (word_vectors: np.array: of float: A matrix representation of gensim word vectors,\n",
    "              index_to_word: list: Index to word mapping,\n",
    "              word_to_index: dict: Word to Index mapping)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load Word Vector Model and get a list of vocab\n",
    "    wv_model = word2vec.load(model_path)\n",
    "    index_to_word = list(wv_model.wv.vocab.keys())\n",
    "   \n",
    "    word_vectors = list()\n",
    "    \n",
    "    # Populate matrix of word vectors\n",
    "    for word in index_to_word:\n",
    "        word_vectors.append(wv_model[word])\n",
    "    \n",
    "    # Add a special words(unknow, start, end)\n",
    "    index_to_word += X_WORDS.values()\n",
    "    \n",
    "    # Create a reverse mapping for words\n",
    "    word_to_index = dict((word, idx) for idx, word in enumerate(index_to_word))    \n",
    "    \n",
    "    for word in X_WORDS:\n",
    "        # A random_vector for special words\n",
    "        random_vector = np.random.rand(wv_model.vector_size)\n",
    "        word_vectors.append(random_vector)\n",
    "    \n",
    "    return np.array(word_vectors), index_to_word, word_to_index\n",
    "\n",
    "\n",
    "def get_word_mappings(documents):\n",
    "    \"\"\"\n",
    "    Returns unique words in a list of strings\n",
    "    \n",
    "    :param documents: list: a list of lists    \n",
    "    :returns (None, index_to_word: list: Index to word mapping,\n",
    "              word_to_index: dict: Word to Index mapping)\n",
    "    \"\"\"\n",
    "    \n",
    "    # If type of documents is a list of words then join them together\n",
    "    if type(documents[0]) == list:\n",
    "        documents = [\" \".join(doc) for doc in documents]\n",
    "        \n",
    "    vocab = (\" \".join(documents).split()) + [X_WORDS[\"unknown\"]] # End tags will already be there\n",
    "    index_to_word = np.unique(vocab)\n",
    "    word_to_index = dict((word, idx) for idx, word in enumerate(index_to_word))\n",
    "    \n",
    "    return None, index_to_word, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"data/penn/train.txt\"\n",
    "TEST_FILE = \"data/penn/test.txt\"\n",
    "\n",
    "train_data = preprocess(open(TRAIN_FILE, 'r').readlines(), boundary_tags=True)\n",
    "test_data = preprocess(open(TEST_FILE, 'r').readlines(), boundary_tags=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>',\n",
       " ' ',\n",
       " 'p',\n",
       " 'l',\n",
       " 'a',\n",
       " 'n',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 'g',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'd',\n",
       " 'v',\n",
       " 'e',\n",
       " 'r',\n",
       " 't',\n",
       " 'i',\n",
       " 's',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " ' ',\n",
       " 'd',\n",
       " 'i',\n",
       " 's',\n",
       " 'c',\n",
       " 'o',\n",
       " 'u',\n",
       " 'n',\n",
       " 't',\n",
       " 's',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'm',\n",
       " 'a',\n",
       " 'i',\n",
       " 'n',\n",
       " 't',\n",
       " 'a',\n",
       " 'i',\n",
       " 'n',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 'c',\n",
       " 'r',\n",
       " 'e',\n",
       " 'a',\n",
       " 's',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'a',\n",
       " 'd',\n",
       " ' ',\n",
       " 's',\n",
       " 'p',\n",
       " 'e',\n",
       " 'n',\n",
       " 'd',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'e',\n",
       " 'r',\n",
       " 'm',\n",
       " 'a',\n",
       " 'n',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " ' ',\n",
       " '<',\n",
       " 'u',\n",
       " 'n',\n",
       " 'k',\n",
       " '>',\n",
       " ' ',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'n',\n",
       " 'e',\n",
       " 'w',\n",
       " 's',\n",
       " ' ',\n",
       " '<',\n",
       " 'u',\n",
       " 'n',\n",
       " 'k',\n",
       " '>',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 'u',\n",
       " 'n',\n",
       " 'd',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " 'c',\n",
       " 'o',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'e',\n",
       " 'r',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'p',\n",
       " 'e',\n",
       " 't',\n",
       " 'i',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 't',\n",
       " 'w',\n",
       " 'e',\n",
       " 'e',\n",
       " 'n',\n",
       " ' ',\n",
       " 'n',\n",
       " 'e',\n",
       " 'w',\n",
       " 's',\n",
       " 'w',\n",
       " 'e',\n",
       " 'e',\n",
       " 'k',\n",
       " ' ',\n",
       " 't',\n",
       " 'i',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'a',\n",
       " 'r',\n",
       " 'n',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 'c',\n",
       " '.',\n",
       " ' ',\n",
       " \"'\",\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'i',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 'm',\n",
       " 'a',\n",
       " 'g',\n",
       " 'a',\n",
       " 'z',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " '<',\n",
       " 'u',\n",
       " 'n',\n",
       " 'k',\n",
       " '>',\n",
       " ' ',\n",
       " 'b',\n",
       " '.',\n",
       " ' ',\n",
       " '<',\n",
       " 'u',\n",
       " 'n',\n",
       " 'k',\n",
       " '>',\n",
       " ' ',\n",
       " \"'\",\n",
       " 's',\n",
       " ' ',\n",
       " 'u',\n",
       " '.',\n",
       " 's',\n",
       " '.',\n",
       " ' ',\n",
       " 'n',\n",
       " 'e',\n",
       " 'w',\n",
       " 's',\n",
       " ' ',\n",
       " '&',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'l',\n",
       " 'd',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'p',\n",
       " 'o',\n",
       " 'r',\n",
       " 't',\n",
       " ' ',\n",
       " '\\n',\n",
       " '<end>']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vectors and indices mappings\n",
    "WORD_VECTORS, INDEX_TO_WORD, WORD_TO_INDEX = get_word_mappings(documents=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(sample[:-1], sample[1:]) for sample in train_data]\n",
    "test_data = [(sample[:-1], sample[1:]) for sample in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [(to_indices(x, WORD_TO_INDEX),\n",
    "               to_indices(y, WORD_TO_INDEX)) for x, y in train_data]\n",
    "TEST_DATA = [(to_indices(x, WORD_TO_INDEX),\n",
    "               to_indices(y, WORD_TO_INDEX)) for x, y in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, index_to_tag, embedding_dim=None, vocab_size=None,\n",
    "                 embeddings=None, dropout=0.5, output_activation=\"tanh\", num_layers=1,\n",
    "                 batch_size=1, bidirectional=False, classifier=True, has_cuda=True):\n",
    "        \"\"\"\n",
    "        LSTM Classifier performs multi class classification and Sequence Tagging.\n",
    "        \n",
    "        :param hidden_dim: int: Number of hidden layers in the LSTM\n",
    "        :param tag_to_index: dict: Mapping of output labels with indices\n",
    "        :param embedding_dim: int: Word embeddings dimension        \n",
    "        :param vocab_size: int: Number of unique words in the dataset\n",
    "        :param embeddings: numpy.matrix: Pre-trained word words\n",
    "        :param dropout: float: dropout value\n",
    "        :param output_activation: str: one of the values from [\"tanh\", \"relu\", \"sigmoid\"]\n",
    "        :param num_layers: int: Number of LSTM layers\n",
    "        :param batch_size: int: Number of samples in one batch\n",
    "        :param bidirectional: bool: Bidirectional LSTM or not\n",
    "        :param classsifier: bool: Is this model a classifier(include softmax)\n",
    "        :param has_cuda: bool: Whether to run this model on gpu or not\n",
    "        \"\"\"        \n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        activations = {\"tanh\": F.tanh, \"relu\": F.relu, \"sigmoid\": F.sigmoid, \"\": False}\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.has_cuda = has_cuda\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.index_to_tag = index_to_tag\n",
    "        self.classifier = classifier\n",
    "        self.output_activation = activations[output_activation]\n",
    "        num_labels = len(index_to_tag)\n",
    "        \n",
    "        # If directional set directions to 2\n",
    "        self.directions = 1\n",
    "        if bidirectional:\n",
    "            self.directions = 2\n",
    "        \n",
    "        # Setup embeddings\n",
    "        if not embeddings is None:\n",
    "            embedding_dim = embeddings.shape[1]\n",
    "            self.word_embeddings = nn.Embedding(*embeddings.shape)\n",
    "            self.word_embeddings.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        elif embedding_dim and vocab_size:\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        else:\n",
    "            print(\"You must provide either a pre-trained word vectors matrix as\\\n",
    "                  'embeddings' or 'embedding_dim' and 'vocab_size'\")\n",
    "            return None\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, dropout=dropout,\n",
    "                            num_layers=num_layers, batch_first=False,\n",
    "                            bidirectional=bidirectional) # Coz I like my batch first ;)\n",
    "        self.h2o = nn.Linear(hidden_dim*self.directions, num_labels) # Concatenates if bi-directional\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        \"\"\"\n",
    "        Initialize the hidden states for LSTM\n",
    "        \n",
    "        :returns : tuple: of (autograd.Variable, autograd.Variable)\n",
    "        \"\"\"\n",
    "        if self.has_cuda:\n",
    "            return (Variable(torch.zeros(self.num_layers*self.directions,\n",
    "                                                  self.batch_size,\n",
    "                                                  self.hidden_dim).cuda()),\n",
    "                    Variable(torch.zeros(self.num_layers*self.directions,\n",
    "                                                  self.batch_size,\n",
    "                                                  self.hidden_dim).cuda()))\n",
    "        else:\n",
    "            return (Variable(torch.zeros(self.num_layers*self.directions,\n",
    "                                                  self.batch_size,\n",
    "                                                  self.hidden_dim)),\n",
    "                    Variable(torch.zeros(self.num_layers*self.directions,\n",
    "                                                  self.batch_size,\n",
    "                                                  self.hidden_dim)))\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"\n",
    "        Forward-Pass for RNN, which returns the probability scores of classes. \n",
    "        \n",
    "        :param tokens: autograd.Variable: a list of indices as torch tensors\n",
    "        \n",
    "        :returns: scores: autograd.Variable: Final score for the model\n",
    "        \"\"\"\n",
    "        embeds = self.drop(self.word_embeddings(tokens))\n",
    "        self.output, self.hidden = self.lstm(embeds.view(len(tokens), 1, -1), self.hidden)\n",
    "        self.hidden = (self.hidden[0].detach(), self.hidden[1].detach())\n",
    "        \n",
    "        final_output = self.h2o(F.tanh(self.drop(self.output.view(len(tokens), -1))))\n",
    "        scores = F.log_softmax(final_output)\n",
    "       \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cuda():\n",
    "    return True if torch.cuda.is_available() else False\n",
    "\n",
    "CUDA = is_cuda()\n",
    "\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(1234)\n",
    "else:\n",
    "    torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_Variable(sequence, has_cuda=is_cuda(), ttype=torch.LongTensor):\n",
    "    \"\"\"\n",
    "    Convert a list of words to list of pytorch tensor variables\n",
    "    \n",
    "    :param tokens: list: of str: a list of words in a sentence\n",
    "    :param has_cuda: bool: does this machine has cuda\n",
    "    :param ttype: torch tensor type\n",
    "    :returns : autograd.Variable\n",
    "    \"\"\"\n",
    "    if has_cuda:\n",
    "        tensor = ttype(sequence).cuda()\n",
    "    else:\n",
    "        tensor = ttype(sequence)\n",
    "        \n",
    "    return Variable(tensor)\n",
    "\n",
    "\n",
    "def get_accuracy(x, y):\n",
    "    \"\"\"\n",
    "    Calculates percent of similar instances among two numpy arrays\n",
    "    \n",
    "    :param x: np.array\n",
    "    :param y: np.array\n",
    "    \n",
    "    :returns accuracy: float\n",
    "    \"\"\"\n",
    "    accuracy = np.sum(x == y)/len(x)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def get_metrics(x, y, num_labels):\n",
    "    \"\"\"\n",
    "    Get F1 Score and accuracy for a predicted and target values.\n",
    "    \n",
    "    :param x: np.array\n",
    "    :param y: np.array\n",
    "    :param num_labels: number of unique labels in dataset\n",
    "    :returns (total_f1_score: float, total_accuracy: float)\n",
    "    \"\"\"    \n",
    "    total_f1_score = 0\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    for inp, out in zip(x, y):        \n",
    "        f1 = f1_score(inp, list(out), labels=np.arange(num_labels), average='macro')\n",
    "        \n",
    "        total_f1_score += f1\n",
    "        total_accuracy += get_accuracy(inp, out)        \n",
    "        \n",
    "    return total_f1_score/len(x), total_accuracy/len(x)\n",
    "\n",
    "\n",
    "def predict(model, x):\n",
    "    \"\"\"\n",
    "    Get the prediction as the class name from trained model.\n",
    "    \n",
    "    :param model: pytorch model\n",
    "    :param x: str: a test document\n",
    "    \n",
    "    :returns tag: int: class id for the input\n",
    "    \"\"\"\n",
    "    # Set model to evalution state to turn off dropout\n",
    "    model.eval()\n",
    "    x = to_Variable(x)\n",
    "    yhat = model(x)\n",
    "    _, tag = yhat.max(1)\n",
    "    \n",
    "    return tag.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def evaluate(model, eval_data, num_labels):\n",
    "    \"\"\"\n",
    "    Evaluates the accuracy for the model in the global scope.\n",
    "    \n",
    "    :param model: PyTorch Model\n",
    "    :param eval_data: tuple: as (inputs, targets)\n",
    "    :param num_labels: number of unique labels in dataset\n",
    "    :returns (f1_score: float, accuracy: float)\n",
    "    \"\"\"    \n",
    "    # Turn on the evaluation state to ignore dropouts\n",
    "    model.eval()\n",
    "    results = [predict(model, x) for x, y in eval_data]\n",
    "    f1_score, accuracy = get_metrics(np.array([y for x, y in eval_data]), results, num_labels)\n",
    "    return f1_score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {'hidden_dim': 128, 'learning_rate': 0.05, \n",
    "           'epochs': 10, 'dropout': 0.5, 'embedding_dim': 8,\n",
    "           'vocab_size': len(INDEX_TO_WORD), 'clip_value': 0.5}\n",
    "\n",
    "model = LSTM(hidden_dim=hparams['hidden_dim'], index_to_tag=INDEX_TO_WORD, \n",
    "             embedding_dim=hparams['embedding_dim'], bidirectional=True,\n",
    "             vocab_size=hparams['vocab_size'], dropout=hparams['dropout'])\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer =  optim.SGD(model.parameters(), lr=hparams['learning_rate'],\n",
    "                                          weight_decay=0.0001, momentum=0.9)\n",
    "\n",
    "if CUDA:\n",
    "    model = model.cuda()\n",
    "    loss_fn = loss_fn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 - Average Loss after 1000 samples: 1.663508\n",
      "Epoch  0 - Average Loss after 2000 samples: 0.629467\n",
      "Epoch  0 - Average Loss after 3000 samples: 0.431769\n",
      "Epoch  0 - Average Loss after 4000 samples: 0.350901\n",
      "Epoch  0 - Average Loss after 5000 samples: 0.311514\n",
      "Epoch  0 - Average Loss after 6000 samples: 0.264453\n",
      "Epoch  0 - Average Loss after 7000 samples: 0.239793\n",
      "Epoch  0 - Average Loss after 8000 samples: 0.230574\n",
      "Epoch  0 - Average Loss after 9000 samples: 0.221363\n",
      "Epoch  0 - Average Loss after 10000 samples: 0.201548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method TensorDescriptorArray.__del__ of <torch.backends.cudnn.TensorDescriptorArray object at 0x7f1fa6b6e6a0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bf/anaconda3/envs/factnlp/lib/python3.6/site-packages/torch/backends/cudnn/__init__.py\", line 133, in __del__\n",
      "    check_error(lib.cudnnDestroyTensorDescriptor(ctypes.c_void_p(ptr)))\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 - Average Loss after 11000 samples: 0.191099\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-44f051055348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Initialize hidden states to zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/factnlp/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/factnlp/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_after = 1000\n",
    "test_after = 20000\n",
    "\n",
    "for epoch in range(hparams['epochs']):\n",
    "\n",
    "    count = 0\n",
    "    avg_loss = 0\n",
    "    epoch_loss = 0\n",
    "    test_f1_score = 0\n",
    "    last_test_f1_score = 0\n",
    "\n",
    "    # Randomly shuffle the dataset\n",
    "    np.random.shuffle(TRAIN_DATA)\n",
    "    np.random.shuffle(TEST_DATA)\n",
    "\n",
    "    for tokens, labels in TRAIN_DATA:\n",
    "\n",
    "        x, y = to_Variable(tokens), to_Variable(labels)        \n",
    "\n",
    "        y_ = model(x)        \n",
    "        loss = loss_fn(y_, y)\n",
    "\n",
    "        # Initialize hidden states to zero\n",
    "        model.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm(model.parameters(), hparams['clip_value'])\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-hparams['learning_rate'], p.grad.data)\n",
    "\n",
    "        loss_value = loss.data.cpu().numpy()\n",
    "        avg_loss += loss_value\n",
    "        epoch_loss += loss_value\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if count%print_after == 0:\n",
    "            print(\"Epoch % d - Average Loss after %d samples: %f\" % (epoch, count,\n",
    "                                                                     avg_loss/print_after))\n",
    "            avg_loss = 0\n",
    "\n",
    "        if count%test_after == 0:\n",
    "            train_f1_score, train_accuracy = evaluate(model, TRAIN_DATA[:len(TEST_DATA)],\n",
    "                                                                        len(WORD_TO_INDEX))\n",
    "            print(\"Epoch % d - Train F1 Score, Accuracy after %d samples: %f, %f\"% (epoch,\n",
    "                                                                                        count,\n",
    "                                                                                        train_f1_score,\n",
    "                                                                                        train_accuracy))\n",
    "\n",
    "            test_f1_score, test_accuracy = evaluate(model, TEST_DATA,\n",
    "                                                    len(WORD_TO_INDEX)) # So that we can use it later\n",
    "            print(\"Epoch % d - Test F1 Score, Accuracy after %d samples: %f, %f\" % (epoch,\n",
    "                                                                                       count,\n",
    "                                                                                       test_f1_score,\n",
    "                                                                                       test_accuracy))\n",
    "            model.train() # Get the model back to training state\n",
    "    \n",
    "    l = (epoch_loss/len(TRAIN_DATA))[0]\n",
    "    print(\"AVERAGE EPOCH LOSS and PERPLEXITY:\", (l, np.exp(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "i\n",
      "f\n",
      "<unk>\n",
      "i\n",
      "<unk>\n",
      "h\n",
      "a\n",
      "d\n",
      "<unk>\n",
      "c\n",
      "o\n",
      "m\n",
      "e\n",
      "<unk>\n",
      "i\n",
      "n\n",
      "t\n",
      "o\n",
      "<unk>\n",
      "f\n",
      "r\n",
      "i\n",
      "d\n",
      "a\n",
      "y\n",
      "<unk>\n",
      "o\n",
      "n\n",
      "<unk>\n",
      "m\n",
      "a\n",
      "r\n",
      "g\n",
      "i\n",
      "n\n",
      "<unk>\n",
      "o\n",
      "r\n",
      "<unk>\n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      "<unk>\n",
      "v\n",
      "e\n",
      "r\n",
      "y\n",
      "<unk>\n",
      "l\n",
      "i\n",
      "t\n",
      "h\n",
      "l\n",
      "e\n",
      "<unk>\n",
      "c\n",
      "a\n",
      "s\n",
      "h\n",
      "<unk>\n",
      "i\n",
      "n\n",
      "<unk>\n",
      "t\n",
      "h\n",
      "e\n",
      "<unk>\n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "f\n",
      "o\n",
      "l\n",
      "i\n",
      "o\n",
      "s\n",
      "<unk>\n",
      "i\n",
      "<unk>\n",
      "w\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      "<unk>\n",
      "n\n",
      "o\n",
      "t\n",
      "<unk>\n",
      "d\n",
      "o\n",
      "<unk>\n",
      "a\n",
      "n\n",
      "y\n",
      "<unk>\n",
      "b\n",
      "u\n",
      "u\n",
      "i\n",
      "n\n",
      "g\n",
      "<unk>\n",
      "<unk>\n",
      "<end>\n"
     ]
    }
   ],
   "source": [
    "for word in predict(model, TEST_DATA[10][0]):\n",
    "    print(INDEX_TO_WORD[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "i\n",
      "f\n",
      "<unk>\n",
      "i\n",
      "<unk>\n",
      "h\n",
      "a\n",
      "d\n",
      "<unk>\n",
      "c\n",
      "o\n",
      "m\n",
      "e\n",
      "<unk>\n",
      "i\n",
      "n\n",
      "t\n",
      "o\n",
      "<unk>\n",
      "f\n",
      "r\n",
      "i\n",
      "d\n",
      "a\n",
      "y\n",
      "<unk>\n",
      "o\n",
      "n\n",
      "<unk>\n",
      "m\n",
      "a\n",
      "r\n",
      "g\n",
      "i\n",
      "n\n",
      "<unk>\n",
      "o\n",
      "r\n",
      "<unk>\n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      "<unk>\n",
      "v\n",
      "e\n",
      "r\n",
      "y\n",
      "<unk>\n",
      "l\n",
      "i\n",
      "t\n",
      "t\n",
      "l\n",
      "e\n",
      "<unk>\n",
      "c\n",
      "a\n",
      "s\n",
      "h\n",
      "<unk>\n",
      "i\n",
      "n\n",
      "<unk>\n",
      "t\n",
      "h\n",
      "e\n",
      "<unk>\n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "f\n",
      "o\n",
      "l\n",
      "i\n",
      "o\n",
      "s\n",
      "<unk>\n",
      "i\n",
      "<unk>\n",
      "w\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      "<unk>\n",
      "n\n",
      "o\n",
      "t\n",
      "<unk>\n",
      "d\n",
      "o\n",
      "<unk>\n",
      "a\n",
      "n\n",
      "y\n",
      "<unk>\n",
      "b\n",
      "u\n",
      "y\n",
      "i\n",
      "n\n",
      "g\n",
      "<unk>\n",
      "<unk>\n",
      "<end>\n"
     ]
    }
   ],
   "source": [
    "for word in TEST_DATA[10][1]:\n",
    "    print(INDEX_TO_WORD[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1e-4\n",
    "ntokens = len(INDEX_TO_WORD)\n",
    "model.hidden = model.init_hidden()\n",
    "inp = Variable(torch.rand(1, 1).mul(ntokens).long(), volatile=True)\n",
    "\n",
    "if CUDA:\n",
    "    inp.data = inp.data.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ \n",
      "& \n",
      "& \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$\n",
      "\n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$\n",
      "\n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$\n",
      "\n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$\n",
      "\n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$ \n",
      "$\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_words = 100\n",
    "\n",
    "for i in range(num_words):\n",
    "    output = model(inp)\n",
    "    word_weights = model.output.squeeze().data.div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    inp.data.fill_(word_idx)\n",
    "    word = INDEX_TO_WORD[word_idx]\n",
    "\n",
    "    print(word + ('\\n' if i % 20 == 19 else ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:factnlp]",
   "language": "python",
   "name": "conda-env-factnlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
