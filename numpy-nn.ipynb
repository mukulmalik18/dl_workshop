{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bf/anaconda3/envs/factnlp/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 3,
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "train_X, test_X, train_y, test_y = train_test_split(data, target, test_size=0.20)"
=======
    "# We are keeping 20% of data samples as test set\n",
    "train_X, test_X, train_y, test_y = train_test_split(data, target, test_size = 0.20)"
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 4,
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 30, 120, 30)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 5,
=======
     "execution_count": 4,
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X), len(test_X), len(train_y), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0].shape"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "idim = train_X[0].shape[0] # input layer \n",
    "hdim = 100 # number of neurons in hidden layer\n",
    "odim = len(np.unique(train_y))\n",
    "\n",
    "alpha = 0.001 # Learning rate\n",
    "reg_lambda = 0.01 # Regularization hyparameter"
=======
    "idim = train_X[0].shape[0] # size of input layer - \"4\"\n",
    "hdim = 100 # size of hidden layers(100 nodes)\n",
    "odim = len(np.unique(train_y)) # size of output layer - \"3\"\n",
    "\n",
    "alpha = 0.001 # Learning rate\n",
    "reg_lambda = 0.01"
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
<<<<<<< HEAD
    "\n",
=======
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
    "model = {'W1': None, 'b1': None, 'W2': None, 'b2': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_prop(model, x):\n",
<<<<<<< HEAD
=======
    "    \n",
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores/np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(model, x):\n",
    "    \n",
    "    probs = forward_prop(model, x)\n",
    "    \n",
    "    targets = -np.log(probs[range(len(train_X)), train_y])\n",
    "    loss = np.sum(targets)\n",
    "    \n",
    "    loss += reg_lambda/2 * (np.sum(np.square(model['W1'])) + np.sum(np.square(model['W2'])))\n",
<<<<<<< HEAD
    "    \n",
=======
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
    "    return 1./len(train_X) * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def predict(model, x):    \n",
    "    probs = forward_prop(model, x) # [0.1, 0.2, 0.7]\n",
    "    return np.argmax(probs, axis=1) # 2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model, x, y):\n",
    "    predictions = predict(model, x)\n",
    "    accuracy = np.sum(predictions == y)/len(x)\n",
=======
    "def predict(model, x):\n",
    "    probs = forward_prop(model, x)\n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "def get_accuracy(model, x, y):    \n",
    "    predictions = predict(model, x)\n",
    "    accuracy = np.sum(y == predictions)/len(x)\n",
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 27,
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainer(hdim, epochs):\n",
    "    \n",
    "    W1 = np.random.rand(idim, hdim)/np.sqrt(idim)\n",
    "    b1 = np.zeros((1, hdim))\n",
    "    W2 = np.random.randn(hdim, odim)/np.sqrt(hdim)\n",
    "    b2 = np.zeros((1, odim))\n",
    "    \n",
    "    model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    \n",
    "    # For whole batch\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        z1 = train_X.dot(W1) + b1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        exp_scores = np.exp(z2)\n",
    "        probs = exp_scores/np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        # Backpropagation\n",
    "        delta3 = probs\n",
    "        delta3[range(len(train_X)), train_y] -= 1\n",
    "        dW2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "        dW1 = np.dot(train_X.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "        \n",
    "        # Add regularization terms\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "        \n",
    "        # Gradient descent\n",
    "        W1 += -alpha * dW1\n",
    "        b1 += -alpha * db1\n",
    "        W2 += -alpha * dW2\n",
    "        b2 += -alpha * db2\n",
    "        \n",
    "        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        print(\"Loss after iteration %d: %f\"%(epoch, get_loss(model, train_X)))\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Accuracy after iteration %d: %f\"%(epoch, get_accuracy(model, test_X, test_y)))\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.416751\n",
      "Accuracy after iteration 0: 0.333333\n",
      "Loss after iteration 1: 1.219249\n",
      "Accuracy after iteration 1: 0.333333\n",
      "Loss after iteration 2: 1.138340\n",
      "Accuracy after iteration 2: 0.333333\n",
      "Loss after iteration 3: 1.106581\n",
      "Accuracy after iteration 3: 0.333333\n",
      "Loss after iteration 4: 1.092612\n",
      "Accuracy after iteration 4: 0.333333\n",
      "Loss after iteration 5: 1.084759\n",
      "Accuracy after iteration 5: 0.333333\n",
      "Loss after iteration 6: 1.078553\n",
      "Accuracy after iteration 6: 0.333333\n",
      "Loss after iteration 7: 1.071956\n",
      "Accuracy after iteration 7: 0.566667\n",
      "Loss after iteration 8: 1.063676\n",
      "Accuracy after iteration 8: 0.600000\n",
      "Loss after iteration 9: 1.052654\n",
      "Accuracy after iteration 9: 0.600000\n",
      "Loss after iteration 10: 1.038330\n",
      "Accuracy after iteration 10: 0.566667\n",
      "Loss after iteration 11: 1.021231\n",
      "Accuracy after iteration 11: 0.433333\n",
      "Loss after iteration 12: 1.002027\n",
      "Accuracy after iteration 12: 0.566667\n",
      "Loss after iteration 13: 0.980634\n",
      "Accuracy after iteration 13: 0.766667\n",
      "Loss after iteration 14: 0.957164\n",
      "Accuracy after iteration 14: 0.733333\n",
      "Loss after iteration 15: 0.931757\n",
      "Accuracy after iteration 15: 0.666667\n",
      "Loss after iteration 16: 0.904822\n",
      "Accuracy after iteration 16: 0.666667\n",
      "Loss after iteration 17: 0.877488\n",
      "Accuracy after iteration 17: 0.666667\n",
      "Loss after iteration 18: 0.850966\n",
      "Accuracy after iteration 18: 0.666667\n",
      "Loss after iteration 19: 0.825704\n",
      "Accuracy after iteration 19: 0.666667\n",
      "Loss after iteration 20: 0.801926\n",
      "Accuracy after iteration 20: 0.666667\n",
      "Loss after iteration 21: 0.779812\n",
      "Accuracy after iteration 21: 0.666667\n",
      "Loss after iteration 22: 0.759417\n",
      "Accuracy after iteration 22: 0.666667\n",
      "Loss after iteration 23: 0.740692\n",
      "Accuracy after iteration 23: 0.666667\n",
      "Loss after iteration 24: 0.723535\n",
      "Accuracy after iteration 24: 0.666667\n",
      "Loss after iteration 25: 0.707818\n",
      "Accuracy after iteration 25: 0.666667\n",
      "Loss after iteration 26: 0.693408\n",
      "Accuracy after iteration 26: 0.666667\n",
      "Loss after iteration 27: 0.680178\n",
      "Accuracy after iteration 27: 0.666667\n",
      "Loss after iteration 28: 0.668008\n",
      "Accuracy after iteration 28: 0.666667\n",
      "Loss after iteration 29: 0.656788\n",
      "Accuracy after iteration 29: 0.666667\n",
      "Loss after iteration 30: 0.646422\n",
      "Accuracy after iteration 30: 0.700000\n",
      "Loss after iteration 31: 0.636820\n",
      "Accuracy after iteration 31: 0.700000\n",
      "Loss after iteration 32: 0.627904\n",
      "Accuracy after iteration 32: 0.733333\n",
      "Loss after iteration 33: 0.619604\n",
      "Accuracy after iteration 33: 0.766667\n",
      "Loss after iteration 34: 0.611856\n",
      "Accuracy after iteration 34: 0.800000\n",
      "Loss after iteration 35: 0.604604\n",
      "Accuracy after iteration 35: 0.800000\n",
      "Loss after iteration 36: 0.597797\n",
      "Accuracy after iteration 36: 0.800000\n",
      "Loss after iteration 37: 0.591390\n",
      "Accuracy after iteration 37: 0.833333\n",
      "Loss after iteration 38: 0.585341\n",
      "Accuracy after iteration 38: 0.833333\n",
      "Loss after iteration 39: 0.579615\n",
      "Accuracy after iteration 39: 0.833333\n",
      "Loss after iteration 40: 0.574177\n",
      "Accuracy after iteration 40: 0.833333\n",
      "Loss after iteration 41: 0.568998\n",
      "Accuracy after iteration 41: 0.833333\n",
      "Loss after iteration 42: 0.564050\n",
      "Accuracy after iteration 42: 0.833333\n",
      "Loss after iteration 43: 0.559310\n",
      "Accuracy after iteration 43: 0.833333\n",
      "Loss after iteration 44: 0.554754\n",
      "Accuracy after iteration 44: 0.866667\n",
      "Loss after iteration 45: 0.550363\n",
      "Accuracy after iteration 45: 0.866667\n",
      "Loss after iteration 46: 0.546119\n",
      "Accuracy after iteration 46: 0.866667\n",
      "Loss after iteration 47: 0.542004\n",
      "Accuracy after iteration 47: 0.866667\n",
      "Loss after iteration 48: 0.538004\n",
      "Accuracy after iteration 48: 0.866667\n",
      "Loss after iteration 49: 0.534105\n",
      "Accuracy after iteration 49: 0.866667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.30486241,  0.33323591,  0.33239873,  0.37238741,  0.45015627,\n",
       "          0.32949622,  0.43323403,  0.00976688],\n",
       "        [-0.26381264,  0.24174935,  0.21464924,  0.11083567,  0.27838223,\n",
       "          0.20571153,  0.06858255, -0.45813213],\n",
       "        [ 0.65869121,  0.02725431,  0.4880519 ,  0.22481988,  0.19362243,\n",
       "          0.01270763,  0.21042148,  0.43220895],\n",
       "        [ 0.26196883,  0.17217082,  0.40954482,  0.2334273 ,  0.00737781,\n",
       "          0.07905092,  0.36964087,  0.56796479]]),\n",
       " 'W2': array([[-0.75995372,  0.37217094,  0.97298034],\n",
       "        [-0.17821232,  0.41875707, -0.13904729],\n",
       "        [-0.62930882,  0.26956885,  0.05582734],\n",
       "        [ 0.34924559, -0.37551355, -0.34090827],\n",
       "        [ 0.45785432, -0.61767969, -0.4537875 ],\n",
       "        [-0.66286692, -1.10758696, -0.26470009],\n",
       "        [-0.74796652, -0.13092703, -0.69060442],\n",
       "        [-0.73938071,  0.79118115,  0.507564  ]]),\n",
       " 'b1': array([[-0.13357456, -0.00132121, -0.00370171,  0.02962799,  0.00324853,\n",
       "         -0.00310896, -0.00296353, -0.11333915]]),\n",
       " 'b2': array([[ 0.0241499 , -0.12109366,  0.09694377]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer(hdim=8, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4821206ae0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(445)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.2000\n",
       " 0.5000\n",
       " 0.7000\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_data = [0.2, 0.5, 0.7]\n",
    "\n",
    "T = torch.Tensor(T_data)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.0000  2.0000  3.0000\n",
       " 0.1000  0.5000  0.8000\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_data = [[1., 2., 3.], [0.1, 0.5, 0.8]]\n",
    "T = torch.Tensor(T_data)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-1.1756 -1.4819  1.3476\n",
       " 0.5897  0.3648  0.0166\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       " -0.3062  0.9278 -0.5884 -1.5004\n",
       "  0.7840 -0.1858  1.9916  0.1958\n",
       "  0.0626 -1.2176 -1.1573 -1.9002\n",
       "\n",
       "(1 ,.,.) = \n",
       " -0.2893 -0.1305 -0.5576 -0.5225\n",
       " -2.0482  0.1358  0.3479 -0.9099\n",
       " -0.1781 -0.8646  2.5528  0.0982\n",
       "[torch.FloatTensor of size 2x3x4]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 5\n",
       " 7\n",
       " 9\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([1., 2., 3.])\n",
    "y = torch.Tensor([4., 5., 6.])\n",
    "\n",
    "z = x + y\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = autograd.Variable(x)\n",
    "y = autograd.Variable(y)\n",
    "\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.function.AddBackward at 0x7f47ecc5d4f8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 9 \n",
       "-0.4280  0.4936 -1.2031  0.5696 -0.0033  0.2333 -0.1068  1.5816  0.9696  0.1626\n",
       "-0.5639 -0.2374  0.3810 -0.7708  1.2260  0.9913  1.0886  0.5312 -0.8237 -0.3245\n",
       "\n",
       "Columns 10 to 11 \n",
       " 1.8717 -0.2910\n",
       "-0.4367  0.7223\n",
       "[torch.FloatTensor of size 2x12]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.randn(2, 3, 4)\n",
    "s.view(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "X_WORDS = {'unknown': \"<UNK>\"}\n",
    "\n",
    "def data_word_mapping(documents):\n",
    "    # If type of documents is a list of words then join them together\n",
    "    if type(documents[0]) == list:\n",
    "        documents = [\" \".join(doc) for doc in documents]\n",
    "        \n",
    "    vocab = (\" \".join(documents).split()) + [X_WORDS[\"unknown\"]] # End tags will already be there\n",
    "    index_to_word = np.unique(vocab)\n",
    "    word_to_index = dict((word, idx) for idx, word in enumerate(index_to_word))\n",
    "    \n",
    "    return index_to_word, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_to_word, word_to_index = data_word_mapping([sample[0] for sample in train_data])\n",
    "index_to_tag, tag_to_index = ['SPANISH', 'ENGLISH'], {'SPANISH': 0, 'ENGLISH': 1}\n",
    "\n",
    "VOCAB_SIZE = len(word_to_index)\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_labels, vocab_size, embedding_dim = 4):\n",
    "        super(NN, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)        \n",
    "        self.i2h = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.h2o = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def avg_vectors(self, vectors):\n",
    "        return torch.mean(vectors, dim=0)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        embeds = self.word_embeddings(tokens)\n",
    "        output = self.i2h(self.avg_vectors(embeds)).view((1, -1))\n",
    "        final_output = self.h2o(F.relu(output))\n",
    "        label_probs = F.log_softmax(final_output)\n",
    "        return label_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_indices(document, to_ix):\n",
    "    indices = list()\n",
    "        \n",
    "    for word in document:\n",
    "        try:\n",
    "            # Look for the word in dict\n",
    "            indices.append(to_ix[word])\n",
    "        except:\n",
    "            # If not found then add a special word for unknown\n",
    "            indices.append(to_ix[X_WORDS[\"unknown\"]])\n",
    "        \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = [(to_indices(x, word_to_index),\n",
    "               to_indices([y], tag_to_index)) for x, y in train_data]\n",
    "test_data = [(to_indices(x, word_to_index),\n",
    "               to_indices([y], tag_to_index)) for x, y in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NN(hidden_dim=16, num_labels=len(index_to_tag),\n",
    "           vocab_size=VOCAB_SIZE, embedding_dim=8)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    x = autograd.Variable(torch.LongTensor(x))\n",
    "    yhat = model(x)\n",
    "    _, tag = yhat.max(1)\n",
    "    \n",
    "    return tag.data.numpy()\n",
    "\n",
    "def get_accuracy(eval_data):\n",
    "    inputs = [sample[0] for sample in eval_data]\n",
    "    outputs = np.array([sample[1] for sample in eval_data])    \n",
    "    results = [predict(x) for x in inputs]\n",
    "    accuracy = np.sum(outputs == results)/len(inputs)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
=======
   "execution_count": 28,
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 0 - Train Accuracy: 0.500000\n",
      "Epoch 0 - Test Accuracy: 0.500000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 0: 0.809317\n",
      "Epoch 1 - Train Accuracy: 0.500000\n",
      "Epoch 1 - Test Accuracy: 0.500000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 1: 0.750506\n",
      "Epoch 2 - Train Accuracy: 0.500000\n",
      "Epoch 2 - Test Accuracy: 0.500000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 2: 0.704560\n",
      "Epoch 3 - Train Accuracy: 0.750000\n",
      "Epoch 3 - Test Accuracy: 0.500000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 3: 0.665048\n",
      "Epoch 4 - Train Accuracy: 1.000000\n",
      "Epoch 4 - Test Accuracy: 0.500000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 4: 0.626848\n",
      "Epoch 5 - Train Accuracy: 1.000000\n",
      "Epoch 5 - Test Accuracy: 0.500000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 5: 0.586026\n",
      "Epoch 6 - Train Accuracy: 1.000000\n",
      "Epoch 6 - Test Accuracy: 0.500000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 6: 0.539924\n",
      "Epoch 7 - Train Accuracy: 1.000000\n",
      "Epoch 7 - Test Accuracy: 0.500000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 7: 0.488683\n",
      "Epoch 8 - Train Accuracy: 1.000000\n",
      "Epoch 8 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 8: 0.434655\n",
      "Epoch 9 - Train Accuracy: 1.000000\n",
      "Epoch 9 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 9: 0.378120\n",
      "Epoch 10 - Train Accuracy: 1.000000\n",
      "Epoch 10 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 10: 0.322276\n",
      "Epoch 11 - Train Accuracy: 1.000000\n",
      "Epoch 11 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 11: 0.273074\n",
      "Epoch 12 - Train Accuracy: 1.000000\n",
      "Epoch 12 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 12: 0.228148\n",
      "Epoch 13 - Train Accuracy: 1.000000\n",
      "Epoch 13 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 13: 0.189377\n",
      "Epoch 14 - Train Accuracy: 1.000000\n",
      "Epoch 14 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 14: 0.158906\n",
      "Epoch 15 - Train Accuracy: 1.000000\n",
      "Epoch 15 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 15: 0.133242\n",
      "Epoch 16 - Train Accuracy: 1.000000\n",
      "Epoch 16 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 16: 0.113212\n",
      "Epoch 17 - Train Accuracy: 1.000000\n",
      "Epoch 17 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 17: 0.096712\n",
      "Epoch 18 - Train Accuracy: 1.000000\n",
      "Epoch 18 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 18: 0.084152\n",
      "Epoch 19 - Train Accuracy: 1.000000\n",
      "Epoch 19 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 19: 0.073386\n",
      "Epoch 20 - Train Accuracy: 1.000000\n",
      "Epoch 20 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 20: 0.064900\n",
      "Epoch 21 - Train Accuracy: 1.000000\n",
      "Epoch 21 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 21: 0.057418\n",
      "Epoch 22 - Train Accuracy: 1.000000\n",
      "Epoch 22 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 22: 0.051703\n",
      "Epoch 23 - Train Accuracy: 1.000000\n",
      "Epoch 23 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 23: 0.046374\n",
      "Epoch 24 - Train Accuracy: 1.000000\n",
      "Epoch 24 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 24: 0.042348\n",
      "Epoch 25 - Train Accuracy: 1.000000\n",
      "Epoch 25 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 25: 0.038499\n",
      "Epoch 26 - Train Accuracy: 1.000000\n",
      "Epoch 26 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 26: 0.035350\n",
      "Epoch 27 - Train Accuracy: 1.000000\n",
      "Epoch 27 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 27: 0.032545\n",
      "Epoch 28 - Train Accuracy: 1.000000\n",
      "Epoch 28 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 28: 0.030108\n",
      "Epoch 29 - Train Accuracy: 1.000000\n",
      "Epoch 29 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 29: 0.027961\n",
      "Epoch 30 - Train Accuracy: 1.000000\n",
      "Epoch 30 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 30: 0.026122\n",
      "Epoch 31 - Train Accuracy: 1.000000\n",
      "Epoch 31 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 31: 0.024369\n",
      "Epoch 32 - Train Accuracy: 1.000000\n",
      "Epoch 32 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 32: 0.022900\n",
      "Epoch 33 - Train Accuracy: 1.000000\n",
      "Epoch 33 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 33: 0.021505\n",
      "Epoch 34 - Train Accuracy: 1.000000\n",
      "Epoch 34 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 34: 0.020331\n",
      "Epoch 35 - Train Accuracy: 1.000000\n",
      "Epoch 35 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 35: 0.019169\n",
      "Epoch 36 - Train Accuracy: 1.000000\n",
      "Epoch 36 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 36: 0.018194\n",
      "Epoch 37 - Train Accuracy: 1.000000\n",
      "Epoch 37 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 37: 0.017258\n",
      "Epoch 38 - Train Accuracy: 1.000000\n",
      "Epoch 38 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 38: 0.016392\n",
      "Epoch 39 - Train Accuracy: 1.000000\n",
      "Epoch 39 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 39: 0.015641\n",
      "Epoch 40 - Train Accuracy: 1.000000\n",
      "Epoch 40 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 40: 0.014932\n",
      "Epoch 41 - Train Accuracy: 1.000000\n",
      "Epoch 41 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 41: 0.014268\n",
      "Epoch 42 - Train Accuracy: 1.000000\n",
      "Epoch 42 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 42: 0.013659\n",
      "Epoch 43 - Train Accuracy: 1.000000\n",
      "Epoch 43 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 43: 0.013100\n",
      "Epoch 44 - Train Accuracy: 1.000000\n",
      "Epoch 44 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 44: 0.012563\n",
      "Epoch 45 - Train Accuracy: 1.000000\n",
      "Epoch 45 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 45: 0.012078\n",
      "Epoch 46 - Train Accuracy: 1.000000\n",
      "Epoch 46 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 46: 0.011617\n",
      "Epoch 47 - Train Accuracy: 1.000000\n",
      "Epoch 47 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 47: 0.011182\n",
      "Epoch 48 - Train Accuracy: 1.000000\n",
      "Epoch 48 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 48: 0.010790\n",
      "Epoch 49 - Train Accuracy: 1.000000\n",
      "Epoch 49 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 49: 0.010405\n"
=======
      "Loss after iteration 0: 1.730792\n",
      "Accuracy after iteration 0: 0.500000\n",
      "Loss after iteration 1: 1.439260\n",
      "Accuracy after iteration 1: 0.500000\n",
      "Loss after iteration 2: 1.279237\n",
      "Accuracy after iteration 2: 0.500000\n",
      "Loss after iteration 3: 1.185159\n",
      "Accuracy after iteration 3: 0.566667\n",
      "Loss after iteration 4: 1.128534\n",
      "Accuracy after iteration 4: 0.700000\n",
      "Loss after iteration 5: 1.094806\n",
      "Accuracy after iteration 5: 0.266667\n",
      "Loss after iteration 6: 1.073356\n",
      "Accuracy after iteration 6: 0.266667\n",
      "Loss after iteration 7: 1.056066\n",
      "Accuracy after iteration 7: 0.266667\n",
      "Loss after iteration 8: 1.036318\n",
      "Accuracy after iteration 8: 0.333333\n",
      "Loss after iteration 9: 1.007945\n",
      "Accuracy after iteration 9: 0.500000\n",
      "Loss after iteration 10: 0.969829\n",
      "Accuracy after iteration 10: 0.500000\n",
      "Loss after iteration 11: 0.936348\n",
      "Accuracy after iteration 11: 0.500000\n",
      "Loss after iteration 12: 0.912907\n",
      "Accuracy after iteration 12: 0.500000\n",
      "Loss after iteration 13: 0.891245\n",
      "Accuracy after iteration 13: 0.500000\n",
      "Loss after iteration 14: 0.869881\n",
      "Accuracy after iteration 14: 0.500000\n",
      "Loss after iteration 15: 0.848784\n",
      "Accuracy after iteration 15: 0.500000\n",
      "Loss after iteration 16: 0.828136\n",
      "Accuracy after iteration 16: 0.500000\n",
      "Loss after iteration 17: 0.808122\n",
      "Accuracy after iteration 17: 0.500000\n",
      "Loss after iteration 18: 0.788880\n",
      "Accuracy after iteration 18: 0.500000\n",
      "Loss after iteration 19: 0.770502\n",
      "Accuracy after iteration 19: 0.500000\n",
      "Loss after iteration 20: 0.753050\n",
      "Accuracy after iteration 20: 0.500000\n",
      "Loss after iteration 21: 0.736553\n",
      "Accuracy after iteration 21: 0.500000\n",
      "Loss after iteration 22: 0.721018\n",
      "Accuracy after iteration 22: 0.500000\n",
      "Loss after iteration 23: 0.706429\n",
      "Accuracy after iteration 23: 0.500000\n",
      "Loss after iteration 24: 0.692759\n",
      "Accuracy after iteration 24: 0.500000\n",
      "Loss after iteration 25: 0.679969\n",
      "Accuracy after iteration 25: 0.500000\n",
      "Loss after iteration 26: 0.668014\n",
      "Accuracy after iteration 26: 0.500000\n",
      "Loss after iteration 27: 0.656845\n",
      "Accuracy after iteration 27: 0.500000\n",
      "Loss after iteration 28: 0.646409\n",
      "Accuracy after iteration 28: 0.500000\n",
      "Loss after iteration 29: 0.636655\n",
      "Accuracy after iteration 29: 0.500000\n",
      "Loss after iteration 30: 0.627533\n",
      "Accuracy after iteration 30: 0.500000\n",
      "Loss after iteration 31: 0.618994\n",
      "Accuracy after iteration 31: 0.500000\n",
      "Loss after iteration 32: 0.610991\n",
      "Accuracy after iteration 32: 0.500000\n",
      "Loss after iteration 33: 0.603481\n",
      "Accuracy after iteration 33: 0.500000\n",
      "Loss after iteration 34: 0.596422\n",
      "Accuracy after iteration 34: 0.500000\n",
      "Loss after iteration 35: 0.589776\n",
      "Accuracy after iteration 35: 0.500000\n",
      "Loss after iteration 36: 0.583509\n",
      "Accuracy after iteration 36: 0.500000\n",
      "Loss after iteration 37: 0.577586\n",
      "Accuracy after iteration 37: 0.500000\n",
      "Loss after iteration 38: 0.571978\n",
      "Accuracy after iteration 38: 0.500000\n",
      "Loss after iteration 39: 0.566657\n",
      "Accuracy after iteration 39: 0.500000\n",
      "Loss after iteration 40: 0.561597\n",
      "Accuracy after iteration 40: 0.500000\n",
      "Loss after iteration 41: 0.556775\n",
      "Accuracy after iteration 41: 0.500000\n",
      "Loss after iteration 42: 0.552167\n",
      "Accuracy after iteration 42: 0.500000\n",
      "Loss after iteration 43: 0.547755\n",
      "Accuracy after iteration 43: 0.500000\n",
      "Loss after iteration 44: 0.543520\n",
      "Accuracy after iteration 44: 0.500000\n",
      "Loss after iteration 45: 0.539444\n",
      "Accuracy after iteration 45: 0.500000\n",
      "Loss after iteration 46: 0.535510\n",
      "Accuracy after iteration 46: 0.500000\n",
      "Loss after iteration 47: 0.531706\n",
      "Accuracy after iteration 47: 0.500000\n",
      "Loss after iteration 48: 0.528015\n",
      "Accuracy after iteration 48: 0.500000\n",
      "Loss after iteration 49: 0.524428\n",
      "Accuracy after iteration 49: 0.500000\n",
      "Loss after iteration 50: 0.520930\n",
      "Accuracy after iteration 50: 0.500000\n",
      "Loss after iteration 51: 0.517513\n",
      "Accuracy after iteration 51: 0.500000\n",
      "Loss after iteration 52: 0.514166\n",
      "Accuracy after iteration 52: 0.500000\n",
      "Loss after iteration 53: 0.510880\n",
      "Accuracy after iteration 53: 0.500000\n",
      "Loss after iteration 54: 0.507648\n",
      "Accuracy after iteration 54: 0.500000\n",
      "Loss after iteration 55: 0.504461\n",
      "Accuracy after iteration 55: 0.500000\n",
      "Loss after iteration 56: 0.501313\n",
      "Accuracy after iteration 56: 0.500000\n",
      "Loss after iteration 57: 0.498199\n",
      "Accuracy after iteration 57: 0.500000\n",
      "Loss after iteration 58: 0.495112\n",
      "Accuracy after iteration 58: 0.533333\n",
      "Loss after iteration 59: 0.492049\n",
      "Accuracy after iteration 59: 0.533333\n",
      "Loss after iteration 60: 0.489004\n",
      "Accuracy after iteration 60: 0.533333\n",
      "Loss after iteration 61: 0.485974\n",
      "Accuracy after iteration 61: 0.566667\n",
      "Loss after iteration 62: 0.482956\n",
      "Accuracy after iteration 62: 0.566667\n",
      "Loss after iteration 63: 0.479947\n",
      "Accuracy after iteration 63: 0.600000\n",
      "Loss after iteration 64: 0.476944\n",
      "Accuracy after iteration 64: 0.633333\n",
      "Loss after iteration 65: 0.473944\n",
      "Accuracy after iteration 65: 0.666667\n",
      "Loss after iteration 66: 0.470947\n",
      "Accuracy after iteration 66: 0.700000\n",
      "Loss after iteration 67: 0.467950\n",
      "Accuracy after iteration 67: 0.733333\n",
      "Loss after iteration 68: 0.464951\n",
      "Accuracy after iteration 68: 0.733333\n",
      "Loss after iteration 69: 0.461950\n",
      "Accuracy after iteration 69: 0.733333\n",
      "Loss after iteration 70: 0.458945\n",
      "Accuracy after iteration 70: 0.733333\n",
      "Loss after iteration 71: 0.455936\n",
      "Accuracy after iteration 71: 0.766667\n",
      "Loss after iteration 72: 0.452922\n",
      "Accuracy after iteration 72: 0.766667\n",
      "Loss after iteration 73: 0.449902\n",
      "Accuracy after iteration 73: 0.766667\n",
      "Loss after iteration 74: 0.446876\n",
      "Accuracy after iteration 74: 0.766667\n",
      "Loss after iteration 75: 0.443844\n",
      "Accuracy after iteration 75: 0.766667\n",
      "Loss after iteration 76: 0.440805\n",
      "Accuracy after iteration 76: 0.766667\n",
      "Loss after iteration 77: 0.437759\n",
      "Accuracy after iteration 77: 0.766667\n",
      "Loss after iteration 78: 0.434706\n",
      "Accuracy after iteration 78: 0.766667\n",
      "Loss after iteration 79: 0.431647\n",
      "Accuracy after iteration 79: 0.766667\n",
      "Loss after iteration 80: 0.428581\n",
      "Accuracy after iteration 80: 0.766667\n",
      "Loss after iteration 81: 0.425509\n",
      "Accuracy after iteration 81: 0.766667\n",
      "Loss after iteration 82: 0.422431\n",
      "Accuracy after iteration 82: 0.800000\n",
      "Loss after iteration 83: 0.419347\n",
      "Accuracy after iteration 83: 0.833333\n",
      "Loss after iteration 84: 0.416259\n",
      "Accuracy after iteration 84: 0.833333\n",
      "Loss after iteration 85: 0.413165\n",
      "Accuracy after iteration 85: 0.833333\n",
      "Loss after iteration 86: 0.410068\n",
      "Accuracy after iteration 86: 0.833333\n",
      "Loss after iteration 87: 0.406967\n",
      "Accuracy after iteration 87: 0.866667\n",
      "Loss after iteration 88: 0.403863\n",
      "Accuracy after iteration 88: 0.866667\n",
      "Loss after iteration 89: 0.400757\n",
      "Accuracy after iteration 89: 0.866667\n",
      "Loss after iteration 90: 0.397650\n",
      "Accuracy after iteration 90: 0.866667\n",
      "Loss after iteration 91: 0.394542\n",
      "Accuracy after iteration 91: 0.900000\n",
      "Loss after iteration 92: 0.391434\n",
      "Accuracy after iteration 92: 0.900000\n",
      "Loss after iteration 93: 0.388327\n",
      "Accuracy after iteration 93: 0.900000\n",
      "Loss after iteration 94: 0.385222\n",
      "Accuracy after iteration 94: 0.900000\n",
      "Loss after iteration 95: 0.382120\n",
      "Accuracy after iteration 95: 0.900000\n",
      "Loss after iteration 96: 0.379022\n",
      "Accuracy after iteration 96: 0.900000\n",
      "Loss after iteration 97: 0.375928\n",
      "Accuracy after iteration 97: 0.933333\n",
      "Loss after iteration 98: 0.372839\n",
      "Accuracy after iteration 98: 0.933333\n",
      "Loss after iteration 99: 0.369757\n",
      "Accuracy after iteration 99: 0.933333\n"
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "for epoch in range(50):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for tokens, labels in train_data:\n",
    "        \n",
    "        # Initialize hidden states to zero\n",
    "        model.zero_grad()\n",
    "                \n",
    "        x, y = autograd.Variable(torch.LongTensor(tokens)), autograd.Variable(torch.LongTensor(labels))     \n",
    "        \n",
    "        y_ = model(x)        \n",
    "        loss = loss_fn(y_, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value = loss.data.numpy()\n",
    "        epoch_loss += loss_value\n",
    "    \n",
    "    print(\"Epoch %d - Train Accuracy: %f\" % (epoch, get_accuracy(train_data)))\n",
    "    print(\"Epoch %d - Test Accuracy: %f\" % (epoch, get_accuracy(test_data)))\n",
    "    print(\"AVERAGE EPOCH LOSS AFTER EPOCH %d: %f\"%  (epoch, epoch_loss/len(train_data)))"
=======
    "model = trainer(8, epochs=100)"
>>>>>>> 9b7ae70973329e6b897e10718e42e639c6b6d716
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:factnlp]",
   "language": "python",
   "name": "conda-env-factnlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
