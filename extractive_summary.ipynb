{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import en\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "from spacy.en.language_data import STOP_WORDS\n",
    "stopwords = {}\n",
    "for word in STOP_WORDS:\n",
    "    stopwords[word]=''\n",
    "    \n",
    "from gensim.models import Word2Vec as wv\n",
    "model = wv.load('models/Insert_Model_Name')\n",
    "\n",
    "from collections import Counter as c\n",
    "import itertools\n",
    "from __future__ import print_function\n",
    "import functools\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Summarizer():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model=model,\n",
    "                 parser=nlp\n",
    "                ):\n",
    "        self.model = model\n",
    "        self.parser = parser\n",
    "    \n",
    "    def tokenizer(self, parsed):\n",
    "        \n",
    "        \"\"\"\n",
    "        Tokenizes a sentence into tokens and returns list of words \n",
    "        and it's mapping to frequencies.\n",
    "        \n",
    "        1. parsed (input): SpaCy parsed text\n",
    "            * format -> <class 'spacy.tokens.doc.Doc'>\n",
    "        \n",
    "        2. words (output): List of 'unique nouns'\n",
    "            * format: list\n",
    "        \n",
    "        3. words_to_localcount (output): Mapping of nouns to their frequency \n",
    "                                         in input text\n",
    "            * format: dict\n",
    "            \n",
    "        4. words_to_globalcount (output): Mapping of nouns to their frequency \n",
    "                                          in word2vec model's vocab\n",
    "            * format: dict\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        words=[]\n",
    "        for sent in parsed.sents:\n",
    "            for token in sent:\n",
    "                if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "                    words.append(token.text.lower())\n",
    "        \n",
    "        word_to_localcount = c(words)\n",
    "        words = list(set(words))\n",
    "        word_to_globalcount = []\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                glob_count = self.model.wv.vocab[word].count\n",
    "            except KeyError:\n",
    "                glob_count = 0\n",
    "            \n",
    "            word_to_globalcount.append((word,glob_count))\n",
    "        \n",
    "        word_to_globalcount = dict(word_to_globalcount)\n",
    "        \n",
    "        return words, word_to_localcount, word_to_globalcount\n",
    "        \n",
    "        \n",
    "    \n",
    "    def semantic_centroids(self, nouns):\n",
    "        \n",
    "        \"\"\"\n",
    "        Finds Semantic Centroids from list of Nouns\n",
    "        \n",
    "        1. nouns (input): List of nouns\n",
    "            * format -> list\n",
    "        \n",
    "        2. noun_to_score (output): Mapping of Nouns to their relecance weight\n",
    "            * format: dict\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        nouns_score = []\n",
    "        \n",
    "        for noun1 in nouns:\n",
    "            try:\n",
    "                model[noun1]\n",
    "                score = 0\n",
    "                for noun2 in nouns:\n",
    "                    if noun1!=noun2:\n",
    "                        try:\n",
    "                            score+=model.similarity(noun1,noun2)\n",
    "                        except:\n",
    "                            pass\n",
    "                            \n",
    "            except KeyError:\n",
    "                score = 1\n",
    "                \n",
    "            nouns_score.append((noun1,score))\n",
    "            \n",
    "        noun_to_score = dict(nouns_score)\n",
    "        \n",
    "        return noun_to_score\n",
    "            \n",
    "            \n",
    "\n",
    "                    \n",
    "                \n",
    "    def get_wordweight(self, parsed):\n",
    "        \n",
    "        \"\"\"\n",
    "        Tokenizes a sentence into tokens and returns list of words \n",
    "        and it's mapping to frequencies.\n",
    "        \n",
    "        1. parsed (input): SpaCy parsed text\n",
    "            * format -> <class 'spacy.tokens.doc.Doc'>\n",
    "        \n",
    "        2. word_to_weight (output): Mapping of Nouns to their respective \n",
    "                                    Frequency and Semantics based weight\n",
    "            * format: dict\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        words,word_to_localcount,word_to_globalcount = self.tokenizer(parsed)\n",
    "        \n",
    "        centroids = self.semantic_centroids(words)\n",
    "        \n",
    "        word_to_weight = []\n",
    "        \n",
    "        for word in words:\n",
    "            weight = np.log(word_to_localcount[word]+1)/\\\n",
    "                np.log((word_to_globalcount[word]+2)**2)*\\\n",
    "                centroids[word]\n",
    "            word_to_weight.append((word,weight))\n",
    "            \n",
    "        word_to_weight = dict(word_to_weight)\n",
    "        \n",
    "        return word_to_weight\n",
    "    \n",
    "    def get_summary(self, text):\n",
    "        \n",
    "        \"\"\"\n",
    "        Tokenizes a sentence into tokens and returns list of words \n",
    "        and it's mapping to frequencies.\n",
    "        \n",
    "        1. text (input): Input text to be summarized\n",
    "            * format -> Unicode\n",
    "        \n",
    "        2. word_to_weight (output): Extractive Summary of the text\n",
    "            * format: Unicode\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        parsed = self.parser(text)\n",
    "        word_to_weight = self.get_wordweight(parsed)\n",
    "        \n",
    "        sents_score=[]\n",
    "        for sent in parsed.sents:\n",
    "            sent_score=0\n",
    "            for word in sent:\n",
    "                try:\n",
    "                    sent_score+=word_to_weight[word.text.lower()]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            sents_score.append((sent_score,sent.text))\n",
    "        return \"\\n\\n\".join([sent for score,sent in sorted(sents_score,reverse=True)[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summ = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary = summ.get_summary(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lack of type information in function signatures combined with support for operator overloading and just-in-time loading of modules at runtime means that the most common type inference algorithms have nothing to work with until the point in the program's execution when the types are known anyway.\n",
      "\n",
      "Really finally, there is a version of Python called Jython that compiles to JVM byte-codes, allowing very simple integration of Python with Java, and which gives Python programmers access to Java's deep and powerful libraries.\n",
      "\n",
      "Java's automatic type conversions are extremely limited, and the compiler insists that objects passed through interfaces be of a type convertible to the target type, either by inheritance or automatic type promotion.\n",
      "\n",
      "In Python, names have no strong binding to their type, and thanks to duck typing, function arguments can be used to pass in any object whose interface supports the operations required by the function.\n",
      "\n",
      "In Python this isn't possible, but there is a superset of Python called Cython that allows users to specify types where having that information could result in faster code being emitted by the compiler.\n"
     ]
    }
   ],
   "source": [
    "print (summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
