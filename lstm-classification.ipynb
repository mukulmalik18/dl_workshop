{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en_core_web_md'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable as Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from scripts.preprocess import *\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import re\n",
    "from gensim.models import Word2Vec as word2vec\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dict of some additional special words\n",
    "X_WORDS = {\"unknown\": \"<unk>\", \"start\": \"<start>\", \"end\": \"<end>\", \"digit\": \"<digit>\"}\n",
    "\n",
    "\n",
    "def add_boundary_tags(tokens):\n",
    "    \"\"\"\n",
    "    Adds start and end tags to list of tokens\n",
    "    \n",
    "    :param tokens: list: list of tokenized words\n",
    "    :returns str: [<start>, w1, w2...., wn, <end>]\n",
    "    \"\"\"\n",
    "    return [X_WORDS[\"start\"]] + tokens + [X_WORDS[\"end\"]]\n",
    "\n",
    "\n",
    "def preprocess(documents, to_lower=True, boundary_tags=False):\n",
    "    \"\"\"\n",
    "    Preprocesses raw text - convert into lowercase add boundary tags\n",
    "    \n",
    "    :param documents: list: of str\n",
    "    :param to_lower: bool: whether to convert text into lowercase(default=True)\n",
    "    :param boundary_tags: bool: whether to keep boundary tags or not(start, end)\n",
    "    :returns processed: list: of list: of str: a list of lists of words\n",
    "    \"\"\"\n",
    "    processed = list() \n",
    "    \n",
    "    for doc in documents:\n",
    "        \n",
    "        # Convert into lowercase if flag is set\n",
    "        if to_lower:\n",
    "            doc = doc.lower()\n",
    "        tokens = doc.split()\n",
    "        if boundary_tags:\n",
    "            tokens = add_boundary_tags(tokens)\n",
    "        processed.append(tokens)\n",
    "        \n",
    "    return processed\n",
    "\n",
    "\n",
    "def to_indices(document, to_ix):\n",
    "    \"\"\"\n",
    "    Converts documents into a list of indices.\n",
    "    \n",
    "    :param documents: list: of list: of str: a list of lists of words\n",
    "    :param to_ix: dict: a word to index mapping\n",
    "    :returns indices: list: of list: of int: a list of lists of word indices\n",
    "    \"\"\"    \n",
    "    indices = list()\n",
    "        \n",
    "    for word in document:\n",
    "        try:\n",
    "            # Look for the word in dict\n",
    "            indices.append(to_ix[word])\n",
    "        except:\n",
    "            # If not found then add a special word for unknown\n",
    "            indices.append(to_ix[X_WORDS[\"unknown\"]])\n",
    "        \n",
    "    return indices\n",
    "\n",
    "\n",
    "def w2v_word_mapping(model_path):\n",
    "    \"\"\"\n",
    "    Returns mapping of words to indices and vice-versa.\n",
    "    In addition to a numpy matrix representation of\n",
    "    pre-trained word vectors with gensim.\n",
    "    \n",
    "    :param model_path: str: Relative path to the pre-trained gensim model    \n",
    "    :returns (word_vectors: np.array: of float: A matrix representation of gensim word vectors,\n",
    "              index_to_word: list: Index to word mapping,\n",
    "              word_to_index: dict: Word to Index mapping)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load Word Vector Model and get a list of vocab\n",
    "    wv_model = word2vec.load(model_path)\n",
    "    index_to_word = list(wv_model.wv.vocab.keys())\n",
    "   \n",
    "    word_vectors = list()\n",
    "    \n",
    "    # Populate matrix of word vectors\n",
    "    for word in index_to_word:\n",
    "        word_vectors.append(wv_model[word])\n",
    "    \n",
    "    # Add a special words(unknow, start, end)\n",
    "    index_to_word += X_WORDS.values()\n",
    "    \n",
    "    # Create a reverse mapping for words\n",
    "    word_to_index = dict((word, idx) for idx, word in enumerate(index_to_word))    \n",
    "    \n",
    "    for word in X_WORDS:\n",
    "        # A random_vector for special words\n",
    "        random_vector = np.random.rand(wv_model.vector_size)\n",
    "        word_vectors.append(random_vector)\n",
    "    \n",
    "    return np.array(word_vectors), index_to_word, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/4 : Loading Training Data...\n",
      "1/4 : Loading Word Vectors Model(400-dim, Wikipedia)...\n",
      "2/4 : Preprocessing Training data...\n"
     ]
    }
   ],
   "source": [
    "print(\"0/4 : Loading Training Data...\")\n",
    "DATA_PATH = '../factnlp/data/bbc/bbc/'\n",
    "\n",
    "def cleaner(document):\n",
    "    \"\"\"\n",
    "    Removes unwanted characters from the document.\n",
    "    \n",
    "    :param document: str\n",
    "    :returns list: of str: Tokenized document\n",
    "    \"\"\"\n",
    "    text = re.sub(\"[^a-z]+\", \" \", document.lower().replace(\"\\n\", \" \"))\n",
    "    text = text.strip().split()\n",
    "    \n",
    "    return text\n",
    "\n",
    "data = list()\n",
    "for root, dirs, files in os.walk(DATA_PATH):\n",
    "     for file in files:\n",
    "        with open(os.path.join(root, file), encoding='iso-8859-1' ,mode=\"r\") as f:\n",
    "            # Some preprocessing\n",
    "            text = f.read()\n",
    "            text = cleaner(text)\n",
    "            data.append((text, root.split(\"/\")[-1]))\n",
    "            \n",
    "train_data, test_data = train_test_split(data, 0.2)\n",
    "\n",
    "print(\"1/4 : Loading Word Vectors Model(400-dim, Wikipedia)...\")\n",
    "W2V_MODEL_PATH = \"../factnlp/models/prod/word2vec/vsmall_wiki.model\"\n",
    "\n",
    "# Get pre-trained word vectors and indices mappings\n",
    "WORD_VECTORS, INDEX_TO_WORD, WORD_TO_INDEX = w2v_word_mapping(model_path=W2V_MODEL_PATH)\n",
    "\n",
    "print(\"2/4 : Preprocessing Training data...\")\n",
    "\n",
    "# Get index mappings for tags/targets/labels(whatever you call them)\n",
    "INDEX_TO_TAG = list(set(chain(*[[d[1]] for d in train_data])))\n",
    "TAG_TO_INDEX = dict((v, i) for i, v in enumerate(INDEX_TO_TAG))\n",
    "\n",
    "TRAIN_DATA = [(to_indices(x, WORD_TO_INDEX),\n",
    "               to_indices([y], TAG_TO_INDEX)) for x, y in train_data]\n",
    "TEST_DATA = [(to_indices(x, WORD_TO_INDEX),\n",
    "               to_indices([y], TAG_TO_INDEX)) for x, y in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, index_to_tag, embedding_dim=None, vocab_size=None,\n",
    "                 embeddings=None, dropout=0.5, output_activation=\"tanh\", num_layers=1,\n",
    "                 batch_size=1, bidirectional=False, classifier=True, has_cuda=True):\n",
    "        \"\"\"\n",
    "        LSTM Classifier performs multi class classification and Sequence Tagging.\n",
    "        \n",
    "        :param hidden_dim: int: Number of hidden layers in the LSTM\n",
    "        :param tag_to_index: dict: Mapping of output labels with indices\n",
    "        :param embedding_dim: int: Word embeddings dimension        \n",
    "        :param vocab_size: int: Number of unique words in the dataset\n",
    "        :param embeddings: numpy.matrix: Pre-trained word words\n",
    "        :param dropout: float: dropout value\n",
    "        :param output_activation: str: one of the values from [\"tanh\", \"relu\", \"sigmoid\"]\n",
    "        :param num_layers: int: Number of LSTM layers\n",
    "        :param batch_size: int: Number of samples in one batch\n",
    "        :param bidirectional: bool: Bidirectional LSTM or not\n",
    "        :param classsifier: bool: Is this model a classifier(include softmax)\n",
    "        :param has_cuda: bool: Whether to run this model on gpu or not\n",
    "        \"\"\"        \n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        activations = {\"tanh\": F.tanh, \"relu\": F.relu, \"sigmoid\": F.sigmoid, \"\": False}\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.has_cuda = has_cuda\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.index_to_tag = index_to_tag\n",
    "        self.classifier = classifier\n",
    "        self.output_activation = activations[output_activation]\n",
    "        num_labels = len(index_to_tag)\n",
    "        \n",
    "        # If directional set directions to 2\n",
    "        self.directions = 1\n",
    "        if bidirectional:\n",
    "            self.directions = 2\n",
    "        \n",
    "        # Setup embeddings\n",
    "        if not embeddings is None:\n",
    "            embedding_dim = embeddings.shape[1]\n",
    "            self.word_embeddings = nn.Embedding(*embeddings.shape)\n",
    "            self.word_embeddings.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        elif embedding_dim and vocab_size:\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        else:\n",
    "            print(\"You must provide either a pre-trained word vectors matrix as\\\n",
    "                  'embeddings' or 'embedding_dim' and 'vocab_size'\")\n",
    "            return None\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, dropout=dropout,\n",
    "                            num_layers=num_layers, batch_first=False,\n",
    "                            bidirectional=bidirectional) # Coz I like my batch first ;)\n",
    "        self.h2o = nn.Linear(hidden_dim*self.directions, num_labels) # Concatenates if bi-directional\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        \"\"\"\n",
    "        Initialize the hidden states for LSTM\n",
    "        \n",
    "        :returns : tuple: of (autograd.Variable, autograd.Variable)\n",
    "        \"\"\"\n",
    "        if self.has_cuda:\n",
    "            return (Variable(torch.zeros(self.num_layers*self.directions,\n",
    "                                                  self.batch_size,\n",
    "                                                  self.hidden_dim).cuda()),\n",
    "                    Variable(torch.zeros(self.num_layers*self.directions,\n",
    "                                                  self.batch_size,\n",
    "                                                  self.hidden_dim).cuda()))\n",
    "        else:\n",
    "            return (Variable(torch.zeros(self.num_layers*self.directions,\n",
    "                                                  self.batch_size,\n",
    "                                                  self.hidden_dim)),\n",
    "                    Variable(torch.zeros(self.num_layers*self.directions,\n",
    "                                                  self.batch_size,\n",
    "                                                  self.hidden_dim)))\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"\n",
    "        Forward-Pass for RNN, which returns the probability scores of classes. \n",
    "        \n",
    "        :param tokens: autograd.Variable: a list of indices as torch tensors\n",
    "        \n",
    "        :returns: scores: autograd.Variable: Final score for the model\n",
    "        \"\"\"\n",
    "        embeds = self.drop(self.word_embeddings(tokens))\n",
    "        output, self.hidden = self.lstm(embeds.view(len(tokens), 1, -1), self.hidden)\n",
    "        self.hidden = (self.hidden[0].detach(), self.hidden[1].detach())\n",
    "        \n",
    "        final_output = self.h2o(F.tanh(self.drop(output[-1])))\n",
    "        scores = F.log_softmax(final_output)\n",
    "       \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cuda():\n",
    "    return True if torch.cuda.is_available() else False\n",
    "\n",
    "CUDA = is_cuda()\n",
    "\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(1234)\n",
    "else:\n",
    "    torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_Variable(sequence, has_cuda=is_cuda(), ttype=torch.LongTensor):\n",
    "    \"\"\"\n",
    "    Convert a list of words to list of pytorch tensor variables\n",
    "    \n",
    "    :param tokens: list: of str: a list of words in a sentence\n",
    "    :param has_cuda: bool: does this machine has cuda\n",
    "    :param ttype: torch tensor type\n",
    "    :returns : autograd.Variable\n",
    "    \"\"\"\n",
    "    if has_cuda:\n",
    "        tensor = ttype(sequence).cuda()\n",
    "    else:\n",
    "        tensor = ttype(sequence)\n",
    "        \n",
    "    return Variable(tensor)\n",
    "\n",
    "\n",
    "def get_accuracy(x, y):\n",
    "    \"\"\"\n",
    "    Calculates percent of similar instances among two numpy arrays\n",
    "    \n",
    "    :param x: np.array\n",
    "    :param y: np.array\n",
    "    \n",
    "    :returns accuracy: float\n",
    "    \"\"\"\n",
    "    accuracy = np.sum(x == y)/len(x)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def get_metrics(x, y, num_labels):\n",
    "    \"\"\"\n",
    "    Get F1 Score and accuracy for a predicted and target values.\n",
    "    \n",
    "    :param x: np.array\n",
    "    :param y: np.array\n",
    "    :param num_labels: number of unique labels in dataset\n",
    "    :returns (total_f1_score: float, total_accuracy: float)\n",
    "    \"\"\"    \n",
    "    total_f1_score = 0\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    for inp, out in zip(x, y):        \n",
    "        f1 = f1_score(inp, list(out), labels=np.arange(num_labels), average='macro')\n",
    "        \n",
    "        total_f1_score += f1\n",
    "        total_accuracy += get_accuracy(inp, out)        \n",
    "        \n",
    "    return total_f1_score/len(x), total_accuracy/len(x)\n",
    "\n",
    "\n",
    "def predict(model, x):\n",
    "    \"\"\"\n",
    "    Get the prediction as the class name from trained model.\n",
    "    \n",
    "    :param model: pytorch model\n",
    "    :param x: str: a test document\n",
    "    \n",
    "    :returns tag: int: class id for the input\n",
    "    \"\"\"\n",
    "    # Set model to evalution state to turn off dropout\n",
    "    model.eval()\n",
    "    x = to_Variable(x)\n",
    "    yhat = model(x)\n",
    "    _, tag = yhat.max(1)\n",
    "    \n",
    "    return tag.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def evaluate(model, eval_data, num_labels):\n",
    "    \"\"\"\n",
    "    Evaluates the accuracy for the model in the global scope.\n",
    "    \n",
    "    :param model: PyTorch Model\n",
    "    :param eval_data: tuple: as (inputs, targets)\n",
    "    :param num_labels: number of unique labels in dataset\n",
    "    :returns (f1_score: float, accuracy: float)\n",
    "    \"\"\"    \n",
    "    # Turn on the evaluation state to ignore dropouts\n",
    "    model.eval()\n",
    "    results = [predict(model, x) for x, y in eval_data]\n",
    "    f1_score, accuracy = get_metrics(np.array([y for x, y in eval_data]), results, num_labels)\n",
    "    return f1_score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {'hidden_dim': 256, 'learning_rate': 0.0001,\n",
    "           'epochs': 10, 'dropout': 0.5, \n",
    "           'clip_value': 0.5}\n",
    "\n",
    "model = LSTM(hidden_dim=hparams['hidden_dim'], index_to_tag=INDEX_TO_WORD,\n",
    "             embeddings=WORD_VECTORS, bidirectional=True,\n",
    "             dropout=hparams['dropout'])\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer =  optim.Adam(model.parameters(), lr=hparams['learning_rate'],\n",
    "                                          weight_decay=0.0001)\n",
    "\n",
    "if CUDA:\n",
    "    model = model.cuda()\n",
    "    loss_fn = loss_fn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 - Average Loss after 50 samples: 13.063604\n",
      "Epoch  0 - Average Loss after 100 samples: 13.004211\n",
      "Epoch  0 - Average Loss after 150 samples: 12.973135\n",
      "Epoch  0 - Average Loss after 200 samples: 12.916888\n",
      "Epoch  0 - Average Loss after 250 samples: 12.864065\n",
      "Epoch  0 - Average Loss after 300 samples: 12.805221\n",
      "Epoch  0 - Average Loss after 350 samples: 12.712360\n",
      "Epoch  0 - Average Loss after 400 samples: 12.640852\n",
      "Epoch  0 - Average Loss after 450 samples: 12.554396\n",
      "Epoch  0 - Average Loss after 500 samples: 12.308525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bf/anaconda3/envs/factnlp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/bf/anaconda3/envs/factnlp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 - Train F1 Score, Accuracy after 500 samples: 0.000001, 0.447191\n",
      "Epoch  0 - Test F1 Score, Accuracy after 500 samples: 0.000001, 0.355056\n",
      "Epoch  0 - Average Loss after 550 samples: 11.968431\n",
      "Epoch  0 - Average Loss after 600 samples: 11.392900\n",
      "Epoch  0 - Average Loss after 650 samples: 10.667994\n",
      "Epoch  0 - Average Loss after 700 samples: 10.095484\n",
      "Epoch  0 - Average Loss after 750 samples: 9.572539\n",
      "Epoch  0 - Average Loss after 800 samples: 9.101742\n",
      "Epoch  0 - Average Loss after 850 samples: 8.613803\n",
      "Epoch  0 - Average Loss after 900 samples: 8.206642\n",
      "Epoch  0 - Average Loss after 950 samples: 7.732092\n",
      "Epoch  0 - Average Loss after 1000 samples: 7.094090\n",
      "Epoch  0 - Train F1 Score, Accuracy after 1000 samples: 0.000000, 0.208989\n",
      "Epoch  0 - Test F1 Score, Accuracy after 1000 samples: 0.000001, 0.278652\n",
      "Epoch  0 - Average Loss after 1050 samples: 6.757212\n",
      "Epoch  0 - Average Loss after 1100 samples: 6.308798\n",
      "Epoch  0 - Average Loss after 1150 samples: 5.090152\n",
      "Epoch  0 - Average Loss after 1200 samples: 4.319957\n",
      "Epoch  0 - Average Loss after 1250 samples: 3.708941\n",
      "Epoch  0 - Average Loss after 1300 samples: 3.041571\n",
      "Epoch  0 - Average Loss after 1350 samples: 2.856681\n",
      "Epoch  0 - Average Loss after 1400 samples: 2.756112\n",
      "Epoch  0 - Average Loss after 1450 samples: 2.711823\n",
      "Epoch  0 - Average Loss after 1500 samples: 2.571858\n",
      "Epoch  0 - Train F1 Score, Accuracy after 1500 samples: 0.000001, 0.292135\n",
      "Epoch  0 - Test F1 Score, Accuracy after 1500 samples: 0.000001, 0.321348\n",
      "Epoch  0 - Average Loss after 1550 samples: 2.315901\n",
      "Epoch  0 - Average Loss after 1600 samples: 2.422473\n",
      "Epoch  0 - Average Loss after 1650 samples: 2.196641\n",
      "Epoch  0 - Average Loss after 1700 samples: 2.279632\n",
      "Epoch  0 - Average Loss after 1750 samples: 2.115187\n",
      "Epoch  1 - Average Loss after 50 samples: 2.130500\n",
      "Epoch  1 - Average Loss after 100 samples: 2.127185\n",
      "Epoch  1 - Average Loss after 150 samples: 2.014522\n",
      "Epoch  1 - Average Loss after 200 samples: 2.016916\n",
      "Epoch  1 - Average Loss after 250 samples: 1.912011\n",
      "Epoch  1 - Average Loss after 300 samples: 1.947591\n",
      "Epoch  1 - Average Loss after 350 samples: 1.851499\n",
      "Epoch  1 - Average Loss after 400 samples: 1.934692\n",
      "Epoch  1 - Average Loss after 450 samples: 1.829519\n",
      "Epoch  1 - Average Loss after 500 samples: 1.921183\n",
      "Epoch  1 - Train F1 Score, Accuracy after 500 samples: 0.000001, 0.240449\n",
      "Epoch  1 - Test F1 Score, Accuracy after 500 samples: 0.000000, 0.215730\n",
      "Epoch  1 - Average Loss after 550 samples: 1.850499\n",
      "Epoch  1 - Average Loss after 600 samples: 1.835926\n",
      "Epoch  1 - Average Loss after 650 samples: 1.790759\n",
      "Epoch  1 - Average Loss after 700 samples: 1.862179\n",
      "Epoch  1 - Average Loss after 750 samples: 1.811342\n",
      "Epoch  1 - Average Loss after 800 samples: 1.771164\n",
      "Epoch  1 - Average Loss after 850 samples: 1.764414\n",
      "Epoch  1 - Average Loss after 900 samples: 1.757367\n",
      "Epoch  1 - Average Loss after 950 samples: 1.722962\n",
      "Epoch  1 - Average Loss after 1000 samples: 1.807585\n",
      "Epoch  1 - Train F1 Score, Accuracy after 1000 samples: 0.000001, 0.292135\n",
      "Epoch  1 - Test F1 Score, Accuracy after 1000 samples: 0.000001, 0.251685\n",
      "Epoch  1 - Average Loss after 1050 samples: 1.762570\n",
      "Epoch  1 - Average Loss after 1100 samples: 1.774425\n",
      "Epoch  1 - Average Loss after 1150 samples: 1.763686\n",
      "Epoch  1 - Average Loss after 1200 samples: 1.738222\n",
      "Epoch  1 - Average Loss after 1250 samples: 1.770211\n",
      "Epoch  1 - Average Loss after 1300 samples: 1.770631\n",
      "Epoch  1 - Average Loss after 1350 samples: 1.634574\n",
      "Epoch  1 - Average Loss after 1400 samples: 1.683443\n",
      "Epoch  1 - Average Loss after 1450 samples: 1.713802\n",
      "Epoch  1 - Average Loss after 1500 samples: 1.672778\n",
      "Epoch  1 - Train F1 Score, Accuracy after 1500 samples: 0.000001, 0.276404\n",
      "Epoch  1 - Test F1 Score, Accuracy after 1500 samples: 0.000001, 0.325843\n",
      "Epoch  1 - Average Loss after 1550 samples: 1.670873\n",
      "Epoch  1 - Average Loss after 1600 samples: 1.679925\n",
      "Epoch  1 - Average Loss after 1650 samples: 1.647065\n",
      "Epoch  1 - Average Loss after 1700 samples: 1.692788\n",
      "Epoch  1 - Average Loss after 1750 samples: 1.747072\n",
      "Epoch  2 - Average Loss after 50 samples: 1.665076\n"
     ]
    }
   ],
   "source": [
    "print_after = 50\n",
    "test_after = 500\n",
    "\n",
    "for epoch in range(hparams['epochs']):\n",
    "\n",
    "    count = 0\n",
    "    avg_loss = 0\n",
    "    epoch_loss = 0\n",
    "    test_f1_score = 0\n",
    "    last_test_f1_score = 0\n",
    "\n",
    "    # Randomly shuffle the dataset\n",
    "    np.random.shuffle(TRAIN_DATA)\n",
    "    np.random.shuffle(TEST_DATA)\n",
    "\n",
    "    for tokens, labels in TRAIN_DATA:\n",
    "\n",
    "        x, y = to_Variable(tokens), to_Variable(labels)        \n",
    "\n",
    "        y_ = model(x)        \n",
    "        loss = loss_fn(y_, y)\n",
    "\n",
    "        # Initialize hidden states to zero\n",
    "        model.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm(model.parameters(), hparams['clip_value'])\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-hparams['learning_rate'], p.grad.data)\n",
    "\n",
    "        loss_value = loss.data.cpu().numpy()\n",
    "        avg_loss += loss_value\n",
    "        epoch_loss += loss_value\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if count%print_after == 0:\n",
    "            print(\"Epoch % d - Average Loss after %d samples: %f\" % (epoch, count,\n",
    "                                                                     avg_loss/print_after))\n",
    "            avg_loss = 0\n",
    "\n",
    "        if count%test_after == 0:\n",
    "            train_f1_score, train_accuracy = evaluate(model, TRAIN_DATA[:len(TEST_DATA)],\n",
    "                                                                        len(WORD_TO_INDEX))\n",
    "            print(\"Epoch % d - Train F1 Score, Accuracy after %d samples: %f, %f\"% (epoch,\n",
    "                                                                                        count,\n",
    "                                                                                        train_f1_score,\n",
    "                                                                                        train_accuracy))\n",
    "\n",
    "            test_f1_score, test_accuracy = evaluate(model, TEST_DATA,\n",
    "                                                    len(WORD_TO_INDEX)) # So that we can use it later\n",
    "            print(\"Epoch % d - Test F1 Score, Accuracy after %d samples: %f, %f\" % (epoch,\n",
    "                                                                                       count,\n",
    "                                                                                       test_f1_score,\n",
    "                                                                                       test_accuracy))\n",
    "            model.train() # Get the model back to training state\n",
    "    \n",
    "    l = (epoch_loss/len(TRAIN_DATA))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATA[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bf/anaconda3/envs/factnlp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/bf/anaconda3/envs/factnlp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.6828568100041003e-07, 0.26292134831460673)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, TRAIN_DATA[:len(TEST_DATA)], len(WORD_TO_INDEX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:factnlp]",
   "language": "python",
   "name": "conda-env-factnlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
