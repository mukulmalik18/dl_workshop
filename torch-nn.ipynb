{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f00b8141df8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.2000\n",
       " 0.5000\n",
       " 0.7000\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a torch.Tensor object with the given data.\n",
    "T_data = [0.2, 0.5, 0.7]\n",
    "T = torch.Tensor(T_data)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.2000  0.5000  0.7000\n",
       " 1.0000  2.0000  3.0000\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a torch.Tensor object with the given data.\n",
    "T_data = [[0.2, 0.5, 0.7], [1., 2., 3.]]\n",
    "T = torch.Tensor(T_data)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.2000\n",
       " 0.5000\n",
       " 0.7000\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       " -2.9718  1.7070 -0.4305 -2.2820  0.5237\n",
       "  0.0004 -1.2039  3.5283  0.4434  0.5848\n",
       "  0.8407  0.5510  0.3863  0.9124 -0.8410\n",
       "  1.2282 -1.8661  1.4146 -1.8781 -0.4674\n",
       "\n",
       "(1 ,.,.) = \n",
       " -0.7576  0.4215 -0.4827 -1.1198  0.3056\n",
       "  1.0386  0.5206 -0.5006  1.2182  0.2117\n",
       " -1.0613 -1.9441 -0.9596  0.5489 -0.9901\n",
       " -0.3826  1.5037  1.8267  0.5561  1.6445\n",
       "\n",
       "(2 ,.,.) = \n",
       "  0.4973 -1.5067  1.7661 -0.3569 -0.1713\n",
       "  0.4068 -0.4284 -1.1299  1.4274 -1.4027\n",
       "  1.4825 -1.1559  1.6190  0.9581  0.7747\n",
       "  0.1940  0.1687  0.3061  1.0743 -1.0327\n",
       "[torch.FloatTensor of size 3x4x5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensor with random data\n",
    "x = torch.randn((3, 4, 5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([1., 2., 3.])\n",
    "y = torch.Tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "  1.0930  0.7769 -1.3128  0.7099\n",
      "  0.9944 -0.2694 -0.6491 -0.1373\n",
      " -0.2954 -0.7725 -0.2215  0.5074\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.6794 -1.6115  0.5230 -0.8890\n",
      "  0.2620  0.0302  0.0013 -1.3987\n",
      "  1.4666 -0.1028 -0.0097 -0.8420\n",
      "[torch.FloatTensor of size 2x3x4]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      " 1.0930  0.7769 -1.3128  0.7099  0.9944 -0.2694 -0.6491 -0.1373 -0.2954 -0.7725\n",
      "-0.6794 -1.6115  0.5230 -0.8890  0.2620  0.0302  0.0013 -1.3987  1.4666 -0.1028\n",
      "\n",
      "Columns 10 to 11 \n",
      "-0.2215  0.5074\n",
      "-0.0097 -0.8420\n",
      "[torch.FloatTensor of size 2x12]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      " 1.0930  0.7769 -1.3128  0.7099  0.9944 -0.2694 -0.6491 -0.1373 -0.2954 -0.7725\n",
      "-0.6794 -1.6115  0.5230 -0.8890  0.2620  0.0302  0.0013 -1.3987  1.4666 -0.1028\n",
      "\n",
      "Columns 10 to 11 \n",
      "-0.2215  0.5074\n",
      "-0.0097 -0.8420\n",
      "[torch.FloatTensor of size 2x12]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reshaping\n",
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "<torch.autograd.function.AddBackward object at 0x7f006c977318>\n"
     ]
    }
   ],
   "source": [
    "# The Variable class keeps track of how it was created. \n",
    "x = autograd.Variable(torch.Tensor([1., 2., 3]), requires_grad=True)\n",
    "# You can access the data with the .data attribute\n",
    "print(x.data)\n",
    "\n",
    "# You can also do all the same operations you did with tensors with Variables.\n",
    "y = autograd.Variable(torch.Tensor([4., 5., 6]), requires_grad=True)\n",
    "z = x + y\n",
    "print(z.data)\n",
    "\n",
    "# BUT z knows something extra.\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 21\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "<torch.autograd.function.SumBackward object at 0x7f006c977228>\n"
     ]
    }
   ],
   "source": [
    "# Lets sum up all the entries in z\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calling .backward() on any variable will run backprop, starting from it.\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable as Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "X_WORDS = {'unknown': \"<UNK>\"}\n",
    "\n",
    "def data_word_mapping(documents):\n",
    "    \"\"\"\n",
    "    Returns unique words in a list of strings\n",
    "    \n",
    "    :param documents: list: a list of strings    \n",
    "    :returns (None, index_to_word: list: Index to word mapping,\n",
    "              word_to_index: dict: Word to Index mapping)\n",
    "    \"\"\"\n",
    "    \n",
    "    # If type of documents is a list of words then join them together\n",
    "    if type(documents[0]) == list:\n",
    "        documents = [\" \".join(doc) for doc in documents]\n",
    "        \n",
    "    vocab = (\" \".join(documents).split()) + [X_WORDS[\"unknown\"]] # End tags will already be there\n",
    "    index_to_word = np.unique(vocab)\n",
    "    word_to_index = dict((word, idx) for idx, word in enumerate(index_to_word))\n",
    "    \n",
    "    return index_to_word, word_to_index\n",
    "\n",
    "index_to_word, word_to_index = data_word_mapping([sample[0] for sample in train_data])\n",
    "index_to_tag, tag_to_index = ['SPANISH', 'ENGLISH'], {'SPANISH': 0, 'ENGLISH': 1}\n",
    "\n",
    "VOCAB_SIZE = len(word_to_index)\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, hidden_dim, num_labels, vocab_size, embedding_dim = 4):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.i2h = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.h2o = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def avg_vectors(self, vectors):\n",
    "        \"\"\"\n",
    "        Get the average vector for a document.\n",
    "        \n",
    "        :param: vectors: torch.FloatTensor: A tensor with word vector for each word\n",
    "        :returns torch.FloatTensor\n",
    "        \"\"\"\n",
    "        return torch.mean(vectors, dim=0)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"\n",
    "        Forward-Pass for LSTM, which returns the probability scores of classes. \n",
    "        \n",
    "        :param tokens: autograd.Variable: a list of indices as torch tensors\n",
    "        \n",
    "        :returns: label_scores: autograd.Variable: probability scores for classes\n",
    "        \"\"\"\n",
    "        embeds = self.word_embeddings(tokens)\n",
    "        output = self.i2h(self.avg_vectors(embeds)).view((1, -1))\n",
    "        final_output = self.h2o(F.relu(output))\n",
    "        label_probs = F.log_softmax(final_output)\n",
    "        return label_probs\n",
    "\n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_indices(document, to_ix):\n",
    "    \"\"\"\n",
    "    Converts documents into a list of indices.\n",
    "    \n",
    "    :param documents: list: of list: of str: a list of lists of words\n",
    "    :param to_ix: dict: a word to index mapping\n",
    "    :returns indices: list: of list: of int: a list of lists of word indices\n",
    "    \"\"\"    \n",
    "    indices = list()\n",
    "        \n",
    "    for word in document:\n",
    "        try:\n",
    "            # Look for the word in dict\n",
    "            indices.append(to_ix[word])\n",
    "        except:\n",
    "            # If not found then add a special word for unknown\n",
    "            indices.append(to_ix[X_WORDS[\"unknown\"]])\n",
    "        \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(to_indices(x, word_to_index),\n",
    "               to_indices([y], tag_to_index)) for x, y in train_data]\n",
    "test_data = [(to_indices(x, word_to_index),\n",
    "               to_indices([y], tag_to_index)) for x, y in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([18, 12, 7, 9, 16, 6], [0]),\n",
       " ([1, 15, 22, 18], [1]),\n",
       " ([2, 8, 20, 21, 23, 5, 13], [0]),\n",
       " ([2, 15, 14, 19, 3, 11, 13, 22, 10, 17, 4, 21], [1])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(hidden_dim=16, num_labels=len(index_to_tag),\n",
    "           vocab_size=VOCAB_SIZE, embedding_dim=8)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    \"\"\"\n",
    "    Get the prediction as the class name from trained model.\n",
    "    \n",
    "    :param model: pytorch model\n",
    "    :param x: str: a test document\n",
    "    \n",
    "    :returns tag: int: class id for the input\n",
    "    \"\"\"\n",
    "    x = Variable(torch.LongTensor(x))\n",
    "    yhat = model(x)\n",
    "    _, tag = yhat.max(1)\n",
    "    \n",
    "    return tag.data.numpy()\n",
    "\n",
    "def get_accuracy(eval_data):\n",
    "    inputs = [sample[0] for sample in eval_data]\n",
    "    outputs = np.array([sample[1] for sample in eval_data])\n",
    "    \n",
    "    results = [predict(x) for x in inputs]\n",
    "    accuracy = np.sum(outputs == results)/len(inputs)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Accuracy: 0.750000\n",
      "Epoch 0 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 0: 0.711156\n",
      "Epoch 1 - Train Accuracy: 1.000000\n",
      "Epoch 1 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 1: 0.659879\n",
      "Epoch 2 - Train Accuracy: 1.000000\n",
      "Epoch 2 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 2: 0.620618\n",
      "Epoch 3 - Train Accuracy: 1.000000\n",
      "Epoch 3 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 3: 0.579139\n",
      "Epoch 4 - Train Accuracy: 1.000000\n",
      "Epoch 4 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 4: 0.538982\n",
      "Epoch 5 - Train Accuracy: 1.000000\n",
      "Epoch 5 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 5: 0.492835\n",
      "Epoch 6 - Train Accuracy: 1.000000\n",
      "Epoch 6 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 6: 0.448303\n",
      "Epoch 7 - Train Accuracy: 1.000000\n",
      "Epoch 7 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 7: 0.404622\n",
      "Epoch 8 - Train Accuracy: 1.000000\n",
      "Epoch 8 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 8: 0.369942\n",
      "Epoch 9 - Train Accuracy: 1.000000\n",
      "Epoch 9 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 9: 0.326177\n",
      "Epoch 10 - Train Accuracy: 1.000000\n",
      "Epoch 10 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 10: 0.291268\n",
      "Epoch 11 - Train Accuracy: 1.000000\n",
      "Epoch 11 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 11: 0.264445\n",
      "Epoch 12 - Train Accuracy: 1.000000\n",
      "Epoch 12 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 12: 0.231389\n",
      "Epoch 13 - Train Accuracy: 1.000000\n",
      "Epoch 13 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 13: 0.205208\n",
      "Epoch 14 - Train Accuracy: 1.000000\n",
      "Epoch 14 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 14: 0.181778\n",
      "Epoch 15 - Train Accuracy: 1.000000\n",
      "Epoch 15 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 15: 0.166344\n",
      "Epoch 16 - Train Accuracy: 1.000000\n",
      "Epoch 16 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 16: 0.143616\n",
      "Epoch 17 - Train Accuracy: 1.000000\n",
      "Epoch 17 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 17: 0.127655\n",
      "Epoch 18 - Train Accuracy: 1.000000\n",
      "Epoch 18 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 18: 0.117197\n",
      "Epoch 19 - Train Accuracy: 1.000000\n",
      "Epoch 19 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 19: 0.102225\n",
      "Epoch 20 - Train Accuracy: 1.000000\n",
      "Epoch 20 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 20: 0.094365\n",
      "Epoch 21 - Train Accuracy: 1.000000\n",
      "Epoch 21 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 21: 0.084323\n",
      "Epoch 22 - Train Accuracy: 1.000000\n",
      "Epoch 22 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 22: 0.074995\n",
      "Epoch 23 - Train Accuracy: 1.000000\n",
      "Epoch 23 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 23: 0.068321\n",
      "Epoch 24 - Train Accuracy: 1.000000\n",
      "Epoch 24 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 24: 0.063585\n",
      "Epoch 25 - Train Accuracy: 1.000000\n",
      "Epoch 25 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 25: 0.056917\n",
      "Epoch 26 - Train Accuracy: 1.000000\n",
      "Epoch 26 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 26: 0.053126\n",
      "Epoch 27 - Train Accuracy: 1.000000\n",
      "Epoch 27 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 27: 0.048318\n",
      "Epoch 28 - Train Accuracy: 1.000000\n",
      "Epoch 28 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 28: 0.045122\n",
      "Epoch 29 - Train Accuracy: 1.000000\n",
      "Epoch 29 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 29: 0.041312\n",
      "Epoch 30 - Train Accuracy: 1.000000\n",
      "Epoch 30 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 30: 0.038584\n",
      "Epoch 31 - Train Accuracy: 1.000000\n",
      "Epoch 31 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 31: 0.036427\n",
      "Epoch 32 - Train Accuracy: 1.000000\n",
      "Epoch 32 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 32: 0.033625\n",
      "Epoch 33 - Train Accuracy: 1.000000\n",
      "Epoch 33 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 33: 0.031568\n",
      "Epoch 34 - Train Accuracy: 1.000000\n",
      "Epoch 34 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 34: 0.029927\n",
      "Epoch 35 - Train Accuracy: 1.000000\n",
      "Epoch 35 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 35: 0.028000\n",
      "Epoch 36 - Train Accuracy: 1.000000\n",
      "Epoch 36 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 36: 0.026582\n",
      "Epoch 37 - Train Accuracy: 1.000000\n",
      "Epoch 37 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 37: 0.024967\n",
      "Epoch 38 - Train Accuracy: 1.000000\n",
      "Epoch 38 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 38: 0.023682\n",
      "Epoch 39 - Train Accuracy: 1.000000\n",
      "Epoch 39 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 39: 0.022456\n",
      "Epoch 40 - Train Accuracy: 1.000000\n",
      "Epoch 40 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 40: 0.021566\n",
      "Epoch 41 - Train Accuracy: 1.000000\n",
      "Epoch 41 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 41: 0.020313\n",
      "Epoch 42 - Train Accuracy: 1.000000\n",
      "Epoch 42 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 42: 0.019575\n",
      "Epoch 43 - Train Accuracy: 1.000000\n",
      "Epoch 43 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 43: 0.018548\n",
      "Epoch 44 - Train Accuracy: 1.000000\n",
      "Epoch 44 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 44: 0.017750\n",
      "Epoch 45 - Train Accuracy: 1.000000\n",
      "Epoch 45 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 45: 0.017090\n",
      "Epoch 46 - Train Accuracy: 1.000000\n",
      "Epoch 46 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 46: 0.016287\n",
      "Epoch 47 - Train Accuracy: 1.000000\n",
      "Epoch 47 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 47: 0.015646\n",
      "Epoch 48 - Train Accuracy: 1.000000\n",
      "Epoch 48 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 48: 0.015128\n",
      "Epoch 49 - Train Accuracy: 1.000000\n",
      "Epoch 49 - Test Accuracy: 1.000000\n",
      "AVERAGE EPOCH LOSS AFTER EPOCH 49: 0.014461\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    \n",
    "    epoch_loss = 0 \n",
    "    \n",
    "    for tokens, labels in train_data:\n",
    "        \n",
    "        # Initialize hidden states to zero\n",
    "        model.zero_grad()\n",
    "                \n",
    "        x, y = Variable(torch.LongTensor(tokens)), Variable(torch.LongTensor(labels))     \n",
    "        \n",
    "        y_ = model(x)        \n",
    "        loss = loss_fn(y_, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value = loss.data.numpy()\n",
    "        epoch_loss += loss_value\n",
    "    \n",
    "    print(\"Epoch %d - Train Accuracy: %f\" % (epoch, get_accuracy(train_data)))\n",
    "    print(\"Epoch %d - Test Accuracy: %f\" % (epoch, get_accuracy(test_data)))\n",
    "    print(\"AVERAGE EPOCH LOSS AFTER EPOCH %d: %f\"%  (epoch, epoch_loss/len(train_data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:factnlp]",
   "language": "python",
   "name": "conda-env-factnlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
